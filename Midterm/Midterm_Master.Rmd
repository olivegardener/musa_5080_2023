---
title: "PPA_Midterm"
author: "Oliver Atwood + Dave Drennan"
date: "2023-10-01"
output: html_document
---

```{r setup, include=FALSE}
rm(list=ls())

knitr::opts_chunk$set(echo = TRUE)

root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"
source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

#some of these are repetitive with above - from Intro_to_ML_Pt`
library(ggplot2)
library(units)
library(httr)
library(rgdal)
library(corrplot)
library(tidyverse)
library(dplyr)
library(sf)
library(spdep)
library(caret)
library(ckanr)
library(FNN)
library(grid)
library(gridExtra)
library(ggcorrplot) # plot correlation plot
library(corrr)      # another way to plot correlation plot
library(kableExtra)
library(jtools)     # for regression model plots
library(ggstance) # to support jtools plots
library(ggpubr)    # plotting R^2 value on ggplot point scatter
library(broom.mixed) # needed for effects plots
library(lubridate)
library(stargazer)

# Coordinate System
coordinate_system <- 2272

Sys.setenv(OGR_GEOJSON_MAX_OBJ_SIZE = "1000000")  # Set to a large value

```

### Load studentData
```{r warning=FALSE}
Houses <- st_read("data/studentData.geojson") %>%
  st_transform(crs = coordinate_system) %>% 
    dplyr::select(sale_price, basements, building_code_description, category_code_description, central_air, exterior_condition,
                  fireplaces, garage_spaces, interior_condition, number_of_bathrooms, number_of_bedrooms, number_of_rooms,
                  number_stories, quality_grade, sale_date, total_area, total_livable_area, type_heater, year_built, 
                  building_code_description_new, musaID, geometry) %>% 
    mutate(age = 2023-year_built) %>% 
    dplyr::select(-year_built)

```

## Read wrangled data
```{r, message=FALSE}
##OA Local files
# TreeCanopy <- st_read("/Users/oliveratwood/Box Sync/PPA Midterm/data/TreeCanopy.geojson")
# landuse <- st_read("/Users/oliveratwood/Box Sync/PPA Midterm/data/Land_Use.geojson")%>%
#   st_transform(crs = coordinate_system)

catchments <- st_read("data/School_Catchment/Catchment_ES_2021-22") %>% st_transform(crs = coordinate_system)
Redlined <- st_read("data/redlining.geojson")
Vacant_Lots <- st_read("data/Vacant_Lots.geojson")
Recycling_Diversion_Rate <- st_read("data/Recycling_Diversion_Rate.geojson")
Hospitals <- st_read("data/Hospitals.geojson")
Regional_Rail_Stations <- st_read("data/Regional_Rail_Stations.geojson")
Existing_Trails <- st_read("data/Existing_Trails.geojson")
Traffic_Collisions <- st_read("data/PhillyHealth_Collisions.geojson")
trees <- st_read("data/trees_OA.geojson")
bike_network <- st_read("data/bike_network.geojson")
parks <- st_read("data/parks.geojson")
uni <- st_read("data/uni.geojson")
neighborhoods <- st_read("data/neighborhoods.geojson")
landcare_lots <- st_read("data/landcare_lots.geojson")
tracts2020 <- st_read("data/tracts2020.geojson")
septa <- st_read("data/septa.geojson")
septa_bus <- septa %>% 
  filter(mode == "Bus")
septa_trolley <- septa %>% 
  filter(mode == "Trolley")
septa_highspeed <- septa %>% 
  filter(mode == "Highspeed")

```

### Sample Data (for refining methodolgy)
```{r}
# Sample Data for testing
# n_sample <- round(nrow(Houses) * 0.1)
# Houses <- Houses[sample(nrow(Houses), n_sample), ]
```

### Extract polygon data to points
```{r}
Houses <- Houses %>% 
          st_join(dplyr::select(Recycling_Diversion_Rate, SCORE)) %>% 
          st_join(dplyr::select(neighborhoods, NAME)) %>%
          st_join(dplyr::select(catchments, ES_Name)) %>%
          st_join(dplyr::select(Redlined, holc_grade)) %>%
          st_join(dplyr::select(tracts2020, MedHHInc, MedRent, HomeOwnRate, PopDensity, PctWhite, PctBach, Car2WorkPct)) %>%
          rename(RecyclingRate = SCORE,
                 Neighborhood = NAME,
                 MedHHInc_ACS = MedHHInc,
                 MedRent_ACS = MedRent,
                 PopDensity_ACS = PopDensity,
                 PctWhite_ACS = PctWhite,
                 PctBach_ACS = PctBach,
                 Car2WorkPct_ACS = Car2WorkPct) %>% 
          select(-HomeOwnRate)

```

### Compute min dist to trails and bike network
```{r}
## Trails
# Compute distances
distances <- st_distance(Houses, Existing_Trails)
# Compute the minimum distance for each point to the nearest line
min_distances <- apply(distances, 1, min)
# Join the minimum distances to the attribute table of the points
Houses$dist2trail <- min_distances

## Bike Network
distances <- st_distance(Houses, bike_network)
min_distances <- apply(distances, 1, min)
Houses$dist2bike <- min_distances

# Parks
distances <- st_distance(Houses, parks)
min_distances <- apply(distances, 1, min)
Houses$dist2park <- min_distances

```

### Compute percent canopy cover within 500ft
```{r warning = FALSE}
# # Create buffers around each house
# HouseBuffers <- st_buffer(Houses, dist = (0.25*5280))
# 
# # Initialize an empty vector to store the canopy coverage percent for each house
# CanopyPct100ft <- vector("numeric", length = nrow(Houses))
# 
# # Loop through each house buffer
# for (i in 1:nrow(HouseBuffers)) {
# # Calculate the intersection area between the current house buffer and the tree canopy
# intersection_area <- sum(st_area(st_intersection(HouseBuffers[i, ], TreeCanopy)))
#   
# # Calculate the canopy coverage percent for the current house buffer
# CanopyPct100ft[i] <- (intersection_area / st_area(HouseBuffers[i, ])) * 100}
# 
# # Add the canopy coverage percent values to the Houses data frame
# Houses$CanopyPctQuMi <- CanopyPct100ft

```

### K nearest neighbors
```{r k nearest neighbors, warning=FALSE}
Houses <- Houses %>%
    mutate(hospitals_nn1 = nn_function(st_coordinates(Houses), 
                              st_coordinates(Hospitals), k = 1),
           rr_station_nn1 = nn_function(st_coordinates(Houses), 
                              st_coordinates(Regional_Rail_Stations), k = 1),
           septa_trolley_nn1 = nn_function(st_coordinates(Houses), 
                              st_coordinates(septa_trolley), k = 1),
           septa_bus_nn5 = nn_function(st_coordinates(Houses), 
                              st_coordinates(septa_bus), k = 5), # look into this?
           septa_highspeed_nn1 = nn_function(st_coordinates(Houses), 
                              st_coordinates(septa_highspeed), k = 1),
           collisions_nn3 = nn_function(st_coordinates(Houses), 
                              st_coordinates(Traffic_Collisions), k = 3), #data old, keep?
           unis_nn1 = nn_function(st_coordinates(Houses), 
                              st_coordinates(st_centroid(uni)), k = 1),
           landcare_lots_nn5 = nn_function(st_coordinates(Houses), 
                              st_coordinates(st_centroid(landcare_lots)), k = 5),
           vacant_lots_nn5 = nn_function(st_coordinates(Houses), 
                              st_coordinates(st_centroid(Vacant_Lots)), k = 5)) %>% 
    filter(sale_price != 0)

```


```{r deliverables packages}
palette5 <- c("#25CB10", "#5AB60C", "#8FA108",   "#C48C04", "#FA7800")

```

Deliverables

# Introduction

# Summary Statistics

```{r stargazer table}

# Convert sf object to data frame
Houses_df <- as.data.frame(Houses) %>%
  select(-musaID)

parameters <- names(Houses_df)

# option parameter can be used to set order based on index
stargazer(Houses_df, type = "text", title = "Table 1: Summary Statistics")


```


# Home Price Scatterplots
Example from book for facetwrap scatterplots

```{r scatterplots, fig.width=9, fig.height=3}

st_drop_geometry(Houses) %>% 
  select(sale_price, vacant_lots_nn5, dist2bike, septa_bus_nn5, collisions_nn3) %>%
  filter(sale_price <= quantile(sale_price, 0.9)) %>%
  gather(Variable, Value, -sale_price) %>% 
   ggplot(aes(log(Value), sale_price)) +
     geom_point(size = .01) + geom_smooth(method = "lm", se=F, colour = "#FA7800") +
     facet_wrap(~Variable, ncol = 4, scales = "free") +
     labs(title = "Price as a function of continuous variables") +
     plotTheme()

```

# Map Home Prices

Follow book - map quintile breaks over neighborhoods 
```{r map home prices}

#book used price per square foot
ggplot() +
  geom_sf(data = neighborhoods, fill = "grey40") +
  geom_sf(data = Houses, aes(colour = q5(sale_price)), 
          show.legend = "point", size = .01) +
  scale_colour_manual(values = palette5,
                   labels=qBr(Houses,"sale_price"),
                   name="Quintile\nBreaks") +
  labs(title="Sale Price, Philly") +
  mapTheme()


```

# 3 Interesting Maps (bonus for something extra engaging)

```{r heatmap1}
ggplot() + geom_sf(data = neighborhoods, fill = "white") +
  stat_density2d(data = data.frame(st_coordinates(trees)), 
                 aes(x=X, y=Y, weight = trees$TREE_DBH, fill = ..level.., alpha = ..level..),
                 size = 0.01, bins = 40, geom = 'polygon') +
  scale_fill_gradient(low = "lightgreen", high = "darkgreen", 
                      breaks=c(0.000000003,0.00000003),
                      labels=c("Minimum","Maximum"), name = "Density") +
  scale_alpha(range = c(0.00, 0.35), guide = FALSE) +
  labs(title = "Weighted Density of Trees, Philly") +
  mapTheme()

```

```{r map2}
ggplot() + geom_sf(data = neighborhoods, fill = "white") +
  stat_density2d(data = data.frame(st_coordinates(Vacant_Lots)), 
                 aes(X, Y, fill = ..level.., alpha = ..level..),
                 size = 0.01, bins = 40, geom = 'polygon') +
  scale_fill_gradient(low = "pink", high = "darkred", 
                      breaks=c(0.000000003,0.00000003),
                      labels=c("Minimum","Maximum"), name = "Density") +
  scale_alpha(range = c(0.00, 0.35), guide = FALSE) +
  labs(title = "Density of Vacant Lots, Philly") +
  mapTheme()
```
+ map for distance to bike lanes
```{r map}
ggplot()+
geom_sf(data = bike_network)


ggplot() + geom_sf(data = neighborhoods, fill = "white") +
  stat_density2d(data = data.frame(st_coordinates(bike_network)), 
                 aes(X, Y, fill = ..level.., alpha = ..level..),
                 size = 0.01, bins = 40, geom = 'polygon') +
  scale_fill_gradient(low = "lightblue", high = "darkblue", 
                      breaks=c(0.000000003,0.00000003),
                      labels=c("Minimum","Maximum"), name = "Density") +
  scale_alpha(range = c(0.00, 0.35), guide = FALSE) +
  geom_sf(data = bike_network, color = "green", alpha = .3)+
  labs(title = "Density of Bike Network, Philly") +
  mapTheme()
```


# Methods

# Table of Results (Training Set)
### oliver to whittle down to statistically relevant vars
### remove challenge data, split into training and test sets

```{r regression training set}
inTrain <- createDataPartition(
              y = paste(boston.sf$Name, boston.sf$NUM_FLOORS.cat, 
                        boston.sf$Style, boston.sf$R_AC), 
              p = .60, list = FALSE)
Houses.training <- boston.sf[inTrain,] 
Houses.test <- boston.sf[-inTrain,]  
 
reg.training <- 
  lm(SalePrice ~ ., data = as.data.frame(boston.training) %>% 
                             dplyr::select(SalePrice, LivingArea, Style, 
                                           GROSS_AREA, NUM_FLOORS.cat,
                                           R_BDRMS, R_FULL_BTH, R_HALF_BTH, 
                                           R_KITCH, R_AC, R_FPLACE, crimes.Buffer))

boston.test <-
  boston.test %>%
  mutate(Regression = "Baseline Regression",
         SalePrice.Predict = predict(reg.training, boston.test),
         SalePrice.Error = SalePrice.Predict - SalePrice,
         SalePrice.AbsError = abs(SalePrice.Predict - SalePrice),
         SalePrice.APE = (abs(SalePrice.Predict - SalePrice)) / SalePrice.Predict)%>%
  filter(SalePrice < 5000000) 

```

Let's try making a model with all of our variables, to start.

```{r model 1}
Houses_Clean <- dplyr::select(Houses, -musaID)

reg1 <- lm(sale_price ~ ., data = select_if(st_drop_geometry(Houses_Clean), is.numeric))
summary(reg1)

```
Let's try narrowing down the variables based on a correlogram
```{r correlation matrix, fig.width=14}
# Select variables of interest and convert them to numeric
vars_of_interest <- select_if(st_drop_geometry(Houses), is.numeric) %>% 
  select(-musaID) %>% 
  na.omit()

ggcorrplot(
  round(cor(vars_of_interest), 2), 
  p.mat = cor_pmat(vars_of_interest),
  colors = c("#FA7800", "white", "#25CB10"),
  type="lower",
  insig = "blank",
  lab = TRUE,  # Ensure labels are shown
  lab_size = 2.5  # Adjust the size of the labels
  ) +  
    labs(title = "Correlation across numeric variables") +
  theme(axis.text = element_text(size = 6)) 

```
From this correlogram, we can see that there are some potentially co-linear variables and variables with low correlation with sale_price. Let's remove these variables and make a model with this smaller set.

```{r model 2}
Houses2 <- Houses_Clean %>% dplyr::select(-number_stories, -age, -PopDensity_ACS, -Car2WorkPct_ACS, -dist2trail, -dist2park, -hospitals_nn1, -rr_station_nn1, -septa_trolley_nn1, -septa_highspeed_nn1, -unis_nn1, -landcare_lots_nn5)

reg2 <- lm(sale_price ~ ., data = select_if(st_drop_geometry(Houses2), is.numeric))
summary(reg2)

```
Our R-squared variable only decreased by 0.01. Not bad at all, given the number of variables removed. There are a lot of variables in this model that appear to have low significance, let's remove them.


```{r model 3}
Houses3 <- Houses2 %>% dplyr::select(-garage_spaces, -MedHHInc_ACS, -MedRent_ACS, -PctWhite_ACS, -dist2bike, -collisions_nn3, -vacant_lots_nn5)

reg3 <- lm(sale_price ~ ., data = select_if(st_drop_geometry(Houses3), is.numeric))
summary(reg3)

```
```{r model 4}
Houses4 <- Houses3 %>% dplyr::select(-RecyclingRate)

reg4 <- lm(sale_price ~ ., data = select_if(st_drop_geometry(Houses4), is.numeric))
summary(reg4)

```
```{r}
Philly_Data <- Houses4
```

# Table of Goodness of Fit (Test Set)

# CV Results

Example from book

```{r cross validation}

fitControl <- trainControl(method = "cv", number = 100)
set.seed(825)

reg.cv <- 
  train(sale_price ~ ., data = st_drop_geometry(training) %>% 
                                dplyr::select(sale_price, 
                                number_stories, 
                                number_of_bathrooms, 
                                number_of_bedrooms), 
     method = "lm", trControl = fitControl, na.action = na.pass)

reg.cv

```

# Pred vs Obs Scatterplot

Example from book

```{r pred and obs}
bothRegressions %>%
  dplyr::select(SalePrice.Predict, SalePrice, Regression) %>%
    ggplot(aes(SalePrice, SalePrice.Predict)) +
  geom_point() +
  stat_smooth(aes(SalePrice, SalePrice), 
             method = "lm", se = FALSE, size = 1, colour="#FA7800") + 
  stat_smooth(aes(SalePrice.Predict, SalePrice), 
              method = "lm", se = FALSE, size = 1, colour="#25CB10") +
  facet_wrap(~Regression) +
  labs(title="Predicted sale price as a function of observed price",
       subtitle="Orange line represents a perfect prediction; Green line represents prediction") +
  plotTheme()

```

# Residuals Map w/ Moran's I

# Predicted Map (Test Set)

# MAPE map by neighborhood

Example from book

```{r mape map}
st_drop_geometry(bothRegressions) %>%
  group_by(Regression, Name) %>%
  summarize(mean.MAPE = mean(SalePrice.APE, na.rm = T)) %>%
  ungroup() %>% 
  left_join(nhoods) %>%
    st_sf() %>%
    ggplot() + 
      geom_sf(aes(fill = mean.MAPE)) +
      geom_sf(data = bothRegressions, colour = "black", size = .5) +
      facet_wrap(~Regression) +
      scale_fill_gradient(low = palette5[1], high = palette5[5],
                          name = "MAPE") +
      labs(title = "Mean test set MAPE by neighborhood") +
      mapTheme()

```

# Scatterplot - MAPE by neighborhood mean price



# Split by Census Groups

Example from book

```{r tracts}
tracts17 <- 
  get_acs(geography = "tract", variables = c("B01001_001E","B01001A_001E","B06011_001"), 
          year = 2017, state=25, county=025, geometry=T, output = "wide") %>%
  st_transform('ESRI:102286')  %>%
  rename(TotalPop = B01001_001E,
         NumberWhites = B01001A_001E,
         Median_Income = B06011_001E) %>%
  mutate(percentWhite = NumberWhites / TotalPop,
         raceContext = ifelse(percentWhite > .5, "Majority White", "Majority Non-White"),
         incomeContext = ifelse(Median_Income > 32322, "High Income", "Low Income"))

grid.arrange(ncol = 2,
  ggplot() + geom_sf(data = na.omit(tracts17), aes(fill = raceContext)) +
    scale_fill_manual(values = c("#25CB10", "#FA7800"), name="Race Context") +
    labs(title = "Race Context") +
    mapTheme() + theme(legend.position="bottom"), 
  ggplot() + geom_sf(data = na.omit(tracts17), aes(fill = incomeContext)) +
    scale_fill_manual(values = c("#25CB10", "#FA7800"), name="Income Context") +
    labs(title = "Income Context") +
    mapTheme() + theme(legend.position="bottom"))

#MAPE for both regressions by race
st_join(bothRegressions, tracts17) %>% 
  group_by(Regression, raceContext) %>%
  summarize(mean.MAPE = scales::percent(mean(SalePrice.APE, na.rm = T))) %>%
  st_drop_geometry() %>%
  spread(raceContext, mean.MAPE) %>%
  kable(caption = "Test set MAPE by neighborhood racial context")

#MAPE for both regressions by income
st_join(bothRegressions, tracts17) %>% 
  filter(!is.na(incomeContext)) %>%
  group_by(Regression, incomeContext) %>%
  summarize(mean.MAPE = scales::percent(mean(SalePrice.APE, na.rm = T))) %>%
  st_drop_geometry() %>%
  spread(incomeContext, mean.MAPE) %>%
  kable(caption = "Test set MAPE by neighborhood income context")

```

# Discussion
## Accuracy

## Generalizability

# Conclusion and Recommendation

