---
title: "PPA_Midterm"
author: "Oliver Atwood + Dave Drennan"
date: "2023-10-11"
output: html_document
---

```{r, include=FALSE}
rm(list=ls())

knitr::opts_chunk$set(echo = TRUE)

root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"
source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

#some of these are repetitive with above - from Intro_to_ML_Pt`
library(ggplot2)
library(units)
library(httr)
library(rgdal)
library(corrplot)
library(tidyverse)
library(tidycensus)
library(dplyr)
library(sf)
library(spdep)
library(caret)
library(ckanr)
library(FNN)
library(grid)
library(gridExtra)
library(ggcorrplot) # plot correlation plot
library(corrr)      # another way to plot correlation plot
library(kableExtra)
library(jtools)     # for regression model plots
library(ggstance) # to support jtools plots
library(ggpubr)    # plotting R^2 value on ggplot point scatter
library(broom.mixed) # needed for effects plots
library(lubridate)
library(stargazer)
library(purrr)
library(tigris)
library(extrafont)


# Coordinate System
coordinate_system <- 2272

options(scipen = 999)

Sys.setenv(OGR_GEOJSON_MAX_OBJ_SIZE = "1000000")  # Set to a large value

palette5 <- c("red", "pink", "grey",   "lightblue", "blue")
palette_sequential <- c("#ffffb2", "#fecc5c", "#fd8d3c", "#f03b20", "#bd0026")
palette_diverging <- c("#e66101", "#fdb863", "#f7f7f7", "#b2abd2", "#5e3c99")
palette_three <- c("#e66101", "#f7f7f7", "#5e3c99")

```

### Load studentData
```{r warning=FALSE}

student_data <- st_read("data/studentData.geojson") %>% 
  st_transform(crs = coordinate_system) 

```

## Read wrangled data
```{r, message=FALSE}

philly_outline <- counties(state = "PA") %>% 
  filter(NAME == "Philadelphia") %>% 
  st_transform(crs = coordinate_system) %>% 
  select(NAME, geometry)

tracts2020 <- st_read("data/tracts2020.geojson") %>% 
  st_transform(crs = coordinate_system)

planning_districts <- st_read("data/Planning_Districts.geojson") %>% 
  st_transform(coordinate_system)

neighborhoods <- st_read("data/neighborhoods.geojson")

redlining <- st_read("data/redlining.geojson") %>% 
  st_transform(crs = coordinate_system) %>%
  mutate(
    holc_grade_val = case_when(
      holc_grade == "Commercial" ~ "0",
      holc_grade == "A" ~ "1",
      holc_grade == "B" ~ "2",
      holc_grade == "C" ~ "3",
      holc_grade == "D" ~ "4",
      )
  )

parks <- st_read("data/parks.geojson") %>% 
  st_transform(crs = coordinate_system)

vacant_lots <- st_read("data/Vacant_Lots.geojson") %>% 
  st_transform(crs = coordinate_system)

trees <- st_read("data/trees.geojson") %>%
  na.omit() %>%
  st_as_sf(coords = c("LOC_X", "LOC_Y"), crs = "EPSG:4326") %>%
  st_transform(crs = coordinate_system) %>% 
  dplyr::select(-TREE_NAME, -TREE_DBH, -YEAR)

septa <- st_read("data/septa.geojson") %>% 
  st_transform(crs = coordinate_system)
light_rail <- septa %>% 
  st_intersection(philly_outline, septa) %>%
  filter(
    (mode == "Highspeed" | mode == "Trolley"), 
    (directionn == "Northbound" | directionn == "Eastbound")
    )

food_access <- st_read("data/NeighborhoodFoodRetail.geojson") %>% 
  st_transform(crs = coordinate_system)

regional_rail <- st_read("data/Regional_Rail_Stations.geojson") %>% 
  st_transform(crs = coordinate_system) %>% 
  st_intersection(philly_outline, regional_rail)

rail_stops <- st_join(light_rail, regional_rail)

```

### Extract polygon data to points
```{r}

model_data <- student_data %>% 
  dplyr::select(musaID, toPredict, sale_price, exterior_condition, fireplaces, interior_condition, number_of_bathrooms, number_of_bedrooms,
                  total_area, total_livable_area, year_built, geometry, ) %>% 
  mutate(age = 2023-year_built) %>% 
  dplyr::select(-year_built)

model_data <- model_data %>% 
          st_join(dplyr::select(redlining, holc_grade_val)) %>%
          # st_join(dplyr::select(planning_districts, DIST_NAME)) %>%
          st_join(dplyr::select(neighborhoods, NAME)) %>%
          st_join(dplyr::select(food_access, TOTAL_HPSS, TOTAL_LPSS)) %>% #HPSS is high produce supply store
          st_join(dplyr::select(tracts2020, MedHHInc, PctWhite, PctBach)) %>%
          rename(
                 MedHHInc_ACS = MedHHInc,
                 PctWhite_ACS = PctWhite,
                 PctBach_ACS = PctBach,
                 neighborhood = NAME,
                 quality_produce_access = TOTAL_HPSS,
                 low_produce_access = TOTAL_LPSS)%>% 
          mutate(PctWhite_ACS = PctWhite_ACS*100,
                 PctBach_ACS = PctBach_ACS*100,
                 holc_grade_val = as.numeric(holc_grade_val)
                 )

# Sample Data for testing
# n_sample <- round(nrow(model_data) * 0.1)
# model_data <- model_data[sample(nrow(model_data), n_sample), ]

```

### Minimum distance
```{r minimum distance}
# Parks
distances <- st_distance(model_data, parks)
min_distances <- apply(distances, 1, min)
model_data$dist2park <- min_distances
```

### K nearest neighbors
```{r k nearest neighbors, warning=FALSE}
model_data <- model_data %>%
    mutate(
      rail_stop_nn1 = nn_function(st_coordinates(model_data), 
                                     st_coordinates(st_centroid(rail_stops)), 
                                     k = 1),
      vacant_lots_nn3 = nn_function(st_coordinates(model_data), 
                                    st_coordinates(st_centroid(vacant_lots)), 
                                    k = 3),
      )

```

### Tree Count
```{r buffer}

model_data$tree.count.buffer <- model_data %>% 
    st_buffer(500) %>% 
    aggregate(mutate(trees, counter = 1),., sum) %>%
    pull(counter)

```


### Impute Values
```{r data na remove}

model_data <- model_data %>%
  mutate(
    exterior_condition = ifelse(is.na(exterior_condition), median(exterior_condition, na.rm = TRUE), exterior_condition),
    interior_condition = ifelse(is.na(interior_condition), median(interior_condition, na.rm = TRUE), interior_condition),
    number_of_bathrooms = ifelse(is.na(number_of_bathrooms), median(number_of_bathrooms, na.rm = TRUE), number_of_bathrooms),
    number_of_bedrooms = ifelse(is.na(number_of_bedrooms), median(number_of_bedrooms, na.rm = TRUE), number_of_bedrooms),
    fireplaces = ifelse(is.na(fireplaces), median(fireplaces, na.rm = TRUE), fireplaces),
    quality_produce_access = ifelse(is.na(quality_produce_access), median(quality_produce_access, na.rm = TRUE), quality_produce_access),
    low_produce_access = ifelse(is.na(low_produce_access), median(low_produce_access, na.rm = TRUE), low_produce_access),
    MedHHInc_ACS = ifelse(is.na(MedHHInc_ACS), median(MedHHInc_ACS, na.rm = TRUE), MedHHInc_ACS),
    PctWhite_ACS = ifelse(is.na(PctWhite_ACS), median(PctWhite_ACS, na.rm = TRUE), PctWhite_ACS),
    total_area = ifelse(is.na(total_area), median(total_area, na.rm = TRUE), total_area),
    tree.count.buffer = ifelse(is.na(tree.count.buffer), median(tree.count.buffer, na.rm = TRUE), tree.count.buffer),
    holc_grade_val = ifelse(is.na(holc_grade_val), median(holc_grade_val, na.rm = TRUE), holc_grade_val),
    age = ifelse(is.na(age), median(age, na.rm = TRUE), age),
    age = ifelse(age<0, 0, age),
    neighborhood = as.factor(neighborhood),
    exterior_condition = as.integer(exterior_condition),
    number_of_bathrooms = as.integer(number_of_bathrooms),
    holc_grade_val = as.integer(holc_grade_val),
    tree.count.buffer = as.integer(tree.count.buffer)) %>%
    filter(sale_price < quantile(sale_price, 0.99))


summary(model_data)
```

Deliverables

# Introduction


# Summary Statistics

```{r stargazer table}

# Convert sf object to data frame
model_data_df <- as.data.frame(model_data) %>%
  select(-musaID)

parameters <- names(model_data_df)

# option parameter can be used to set order based on index
stargazer(model_data_df, type = "text", title = "Table 1: Summary Statistics")

```

## Correlation matrix
```{r correlation matrix, fig.width=14}
# Select variables of interest and convert them to numeric
vars_of_interest <- select_if(st_drop_geometry(model_data), is.numeric) %>% 
  select(-musaID) %>% 
  na.omit()

ggcorrplot(
  round(cor(vars_of_interest), 2), 
  p.mat = cor_pmat(vars_of_interest),
  colors = palette_three,
  type="lower",
  insig = "blank",
  lab = TRUE,  # Ensure labels are shown
  lab_size = 2.5  # Adjust the size of the labels
  ) +  
    labs(title = "Correlation across numeric variables") +
  theme(axis.text = element_text(size = 6)) 

```


# Home Price Scatterplots
Example from book for facetwrap scatterplots

```{r scatterplots, fig.width=6, fig.height=4}

st_drop_geometry(model_data) %>% 
  select(sale_price, vacant_lots_nn3, dist2park, quality_produce_access, tree.count.buffer) %>%
  gather(Variable, Value, -sale_price) %>% 
   ggplot(aes(Value, sale_price)) +
     geom_point(size = .01) + geom_smooth(method = "lm", se=F, colour = "#FA7800") +
     facet_wrap(~Variable, ncol = 2, scales = "free") +
     labs(title = "Price as a function of continuous variables") +
     plotTheme()

```

# Map Home Prices

```{r, fig.width=10}
salepricemap <- ggplot() +
  geom_sf(data = neighborhoods, fill = "white") +
  geom_sf(data = model_data, 
          aes(colour = sale_price), 
          show.legend = "point", size = .001) +
  scale_colour_gradientn(colors = palette_sequential,
                         name="Sale Price",
                         labels = scales::label_number(scale = 1),
                         guide = guide_colourbar(title.position = "top", title.hjust = 0.5)) +
  labs(title="Sale Price, Philly") +
  mapTheme()

salepricemap

```

# 3 Interesting Maps (bonus for something extra engaging)

```{r heatmap1, fig.width=10}
ggplot() + geom_sf(data = neighborhoods, fill = "white") +
  stat_density2d(data = data.frame(st_coordinates(trees)), 
                 aes(X, Y, fill = ..level.., alpha = ..level..),
                 size = 0.01, bins = 40, geom = 'polygon') +
  scale_fill_gradient(low = "lightgreen", high = "darkgreen", 
                      breaks=c(0.000000003,0.00000003),
                      labels=c("Minimum","Maximum"), name = "Density") +
  scale_alpha(range = c(0.00, 0.35), guide = FALSE) +
  labs(title = "Density of Trees, Philly") +
  mapTheme()

```

```{r map2, fig.width=10}
ggplot() + geom_sf(data = food_access, aes(fill = as.numeric(TOTAL_HPSS))) +
  scale_fill_gradient(low = "lavender", high = "purple", name = "Store Count")+
  labs(title = "High Quality Produce Access, Philly",
       subtitle = "Data provided per block group") +
  mapTheme()

```

```{r}
# Extracting centroids and converting to data.frame
centroids <- st_centroid(vacant_lots)
centroids_df <- data.frame(st_coordinates(centroids))

# Plotting
ggplot() + 
  geom_sf(data = neighborhoods, fill = "white") +
  stat_density2d(data = centroids_df, 
                 aes(X, Y, fill = ..level.., alpha = ..level..),
                 size = 0.01, bins = 40, geom = 'polygon') +
  scale_fill_gradient(low = "pink", high = "red", 
                      breaks=c(0.000000003,0.00000003),
                      labels=c("Minimum","Maximum"), name = "Density") +
  scale_alpha(range = c(0.00, 0.35), guide = FALSE) +
  labs(title = "Density of Vacant Lots, Philly") +
  theme_minimal()
```


```{r, fig.width=10}
ggplot() +
  geom_sf(data = neighborhoods, fill = "white") +
  geom_sf(data = model_data %>% filter(vacant_lots_nn3 < quantile(vacant_lots_nn3, 0.99)), 
          aes(colour = vacant_lots_nn3), 
          show.legend = "point", size = .001) +
  scale_colour_gradientn(colors = palette_sequential,
                         name="Vacant Lot Proximity",
                         labels = scales::label_number(scale = 1),
                         guide = guide_colourbar(title.position = "top", title.hjust = 0.5),
                         trans = "reverse") +
  labs(title="Proximity to Vacancy, Philly") +
  mapTheme()


```
This appears to correlate spatially with the more expensive homes in the northeast and northwest of the city, but given the high levels of vacant lots in center city, which coincide with high housing prices, this may not be the best predictor


```{r regression}

Philly_Data <- model_data %>%
  filter(toPredict == "MODELLING",
         age < 500) %>%
  dplyr::select(-toPredict,)

regression <- lm(sale_price ~ ., data = as.data.frame(st_drop_geometry(Philly_Data)))

summary(regression)

```

# Table of Results (Training Set)
### remove challenge data, split into training and test sets

```{r regression training set, warning=FALSE}
inTrain <- createDataPartition(
              y = paste(Philly_Data$neighborhood),
              p = .70, list = FALSE)
Philly_Data.training <- st_drop_geometry(Philly_Data[inTrain,])
Philly_Data.test <- Philly_Data[-inTrain,]  
 
reg.training <- 
  lm(sale_price ~ ., data = as.data.frame(Philly_Data.training))

```

# Table of Goodness of Fit (Test Set)

```{r goodness of fit}

Philly_Data.test <-
  Philly_Data.test %>%
  mutate(sale_price.Predict = predict(reg.training, Philly_Data.test),
         sale_price.Error = sale_price.Predict - sale_price,
         sale_price.AbsError = abs(sale_price.Predict - sale_price),
         sale_price.APE = abs((sale_price.Predict - sale_price) / sale_price.Predict))
 
st_drop_geometry(Philly_Data.test) %>%
  select(sale_price.Predict, sale_price.Error, sale_price.AbsError, sale_price.APE) %>%
  kbl()


st_drop_geometry(Philly_Data.test) %>%
  gather(Variable, Value, -neighborhood) %>%
  filter(Variable == "sale_price.AbsError" | Variable == "sale_price.APE") %>%
  group_by(Variable) %>%
    summarize(meanValue = mean(Value, na.rm = T)) %>%
    spread(Variable, meanValue) %>%
    kable()
```


# CV Results

```{r cross validation, warning-FALSE}

fitControl <- trainControl(method = "cv", number = 100)
set.seed(825)

reg.cv <- 
  train(sale_price ~ ., data = st_drop_geometry(Philly_Data.training),
     method = "lm", trControl = fitControl, na.action = na.pass)

reg.cv

plot1 <- reg.cv$resample

ggplot()+
  geom_histogram(data=plot1, aes(x = MAE))

```

# Pred vs Obs Scatterplot

```{r pred and obs}

Philly_Data.test %>%
  dplyr::select(sale_price.Predict, sale_price) %>%
    ggplot(aes(sale_price, sale_price.Predict)) +
  geom_point() +
  stat_smooth(aes(sale_price.Predict, sale_price), 
              method = "lm", se = FALSE, size = 1, colour="#25CB10") +
  stat_smooth(aes(sale_price, sale_price), 
             method = "lm", se = FALSE, size = 1, colour="#FA7800") + 
  labs(title="Predicted sale price as a function of observed price",
       subtitle="Orange line represents a perfect prediction; Green line represents prediction") +
  plotTheme()

```



# Residuals Map w/ Moran's I
DAVE

```{r residuals plot, fig.width=10}

ggplot() +
  geom_sf(data = neighborhoods, fill = "white") +
  geom_sf(data = Philly_Data.test, aes(colour = q5(sale_price.Error)), 
          show.legend = "point", size = .75) +
  scale_colour_manual(values = palette5,
                   
                   name="Quintile\nBreaks") +
  labs(title="Residuals, Philly") +
  mapTheme()

```
  

```{r spatial lag in errors map}

coords.test <-  st_coordinates(Philly_Data.test)

neighborList.test <- knn2nb(knearneigh(coords.test, 5))

spatialWeights.test <- nb2listw(neighborList.test, style="W")

Philly_Data.test %>%
  mutate(lagPriceError = lag.listw(spatialWeights.test, sale_price.Error)) %>%
  ggplot()+
  geom_point(aes(x =lagPriceError, y =sale_price.Error))
```


```{r moran}

moranTest <- moran.mc(Philly_Data.test$sale_price.Error,
                      spatialWeights.test, nsim = 999)


ggplot(as.data.frame(moranTest$res[c(1:999)]), aes(moranTest$res[c(1:999)])) +
  geom_histogram(binwidth = 0.01) +
  geom_vline(aes(xintercept = moranTest$statistic), colour = "#FA7800",size=1) +
  scale_x_continuous(limits = c(-1, 1)) +
  labs(title="Observed and permuted Moran's I",
       subtitle= "Observed Moran's I in orange",
       x="Moran's I",
       y="Count") +
  plotTheme()

```
# Predicted Map (Test Set)

```{r pred map, fig.width=10}

# Ensure that 'Philly_Data.challenge' only contains the observations you want to predict
Philly_Data.challenge <- model_data

# Generate predictions only for 'Philly_Data.challenge'
Philly_Data.challenge <- Philly_Data.challenge %>%
  mutate(sale_price_prediction = predict(reg.training, .))

# Determine global limits
global_min <- min(min(model_data$sale_price), min(Philly_Data.challenge$sale_price_prediction))
global_max <- max(max(model_data$sale_price), max(Philly_Data.challenge$sale_price_prediction))

# Your ggplot code for salepricemap
salepricemap <- ggplot() +
  geom_sf(data = neighborhoods, fill = "white") +
  geom_sf(data = model_data %>% filter(sale_price < quantile(sale_price, 0.99)), 
          aes(colour = sale_price), 
          show.legend = "point", size = .001) +
  scale_colour_gradientn(colors = palette_sequential,
                         name="Sale Price",
                         labels = scales::label_number(scale = 1),
                         guide = guide_colourbar(title.position = "top", title.hjust = 0.5),
                         limits = c(global_min, global_max)) +  # Set limits
  labs(title="Sale Price, Philly") +
  mapTheme()

# Your ggplot code for predictedsalepricemap
predictedsalepricemap <- ggplot() +
  geom_sf(data = neighborhoods, fill = "white") +
  geom_sf(data = Philly_Data.challenge, aes(colour = sale_price_prediction),
          show.legend = "point", size = .01) +
  scale_colour_gradientn(colors = palette_sequential,
                         name="Predicted Sale Price",
                         labels = scales::label_number(scale = 1),
                         guide = guide_colourbar(title.position = "top", title.hjust = 0.5),
                         limits = c(global_min, global_max)) +  # Set limits
  labs(title="Predicted Sale Price, Philly") +
  mapTheme()

# Arrange the plots side by side
grid.arrange(salepricemap, predictedsalepricemap, ncol=2)

```

```{r export csv}
Philly_Data.challenge.csv <- st_drop_geometry(Philly_Data.challenge) %>%
  filter(toPredict == "CHALLENGE") %>%
  dplyr::select(musaID, sale_price_prediction) %>%
  mutate(teamName = "House It Going")

write.csv(Philly_Data.challenge.csv, "HouseItGoing.csv", row.names = FALSE)

```


# MAPE map by neighborhood

Example from book

```{r mape map}
# Spatial join
Philly_Data.test <- st_join(Philly_Data.test, neighborhoods, join = st_within)


# Grouping and summarization
mean_MAPE_by_neighborhood <- st_drop_geometry(Philly_Data.test) %>%
  group_by(neighborhood) %>%
  summarise(mean.MAPE = mean(sale_price.APE, na.rm = TRUE)) %>%
  ungroup()

# Ensure that "neighborhood" in mean_MAPE_by_neighborhood matches the identifier in neighborhoods
neighborhoods <- neighborhoods %>%
  left_join(mean_MAPE_by_neighborhood, by = c("NAME" = "neighborhood"))


MAPE.Map <- ggplot() + 
  geom_sf(data = neighborhoods, aes(fill = mean.MAPE)) +
  scale_fill_gradient(low = "lightblue", high = "red",
                        name = "MAPE") +
  labs(title = "Mean test set MAPE by neighborhood") +
  mapTheme()

MAPE.Map

```
This shows that the model is good at predicting across much of the city, with errors clustered in one neighborhood. Why is the absolute percent error so high in that neighborhood?


# Scatterplot - MAPE by neighborhood as a function of mean price by neighborhood
```{r}
# Calculate mean price by neighborhood
mean_price_by_neighborhood <- st_drop_geometry(Philly_Data.test) %>%
  group_by(neighborhood) %>%
  summarise(mean_price = mean(sale_price, na.rm = TRUE)) %>%
  ungroup()

# Merge mean_price_by_neighborhood and mean_MAPE_by_neighborhood
neighborhood_summary <- left_join(mean_price_by_neighborhood, mean_MAPE_by_neighborhood, by = "neighborhood")

# Create a scatterplot of MAPE by neighborhood as a function of mean price by neighborhood
scatterplot <- ggplot(neighborhood_summary, aes(x = mean_price, y = mean.MAPE)) +
  geom_point(aes(color = mean.MAPE), size = 1) +
  scale_color_gradient(low = "lightgreen", high = "red") +
  labs(
    title = "Scatterplot of MAPE by Neighborhood as a Function of Mean Price",
    x = "Mean Price by Neighborhood",
    y = "Mean MAPE by Neighborhood",
    color = "MAPE"
  ) +
  theme_minimal()

# Display the scatterplot
print(scatterplot)


```


# Split by Census Groups

Example from book

```{r tracts}
tracts20 <- 
  get_acs(geography = "tract", variables = c("B01001_001E","B01001A_001E","B06011_001"), 
          year = 2020, state="PA", county="Philadelphia", geometry=T, output = "wide") %>%
  st_transform(crs=2272)  %>%
  rename(TotalPop = B01001_001E,
         NumberWhites = B01001A_001E,
         Median_Income = B06011_001E) %>%
  mutate(percentWhite = NumberWhites / TotalPop,
         raceContext = ifelse(percentWhite > .5, "Majority White", "Majority Non-White"),
         incomeContext = ifelse(Median_Income > 32322, "High Income", "Low Income"))

grid.arrange(ncol = 2,
  ggplot() + geom_sf(data = na.omit(tracts20), aes(fill = raceContext)) +
    scale_fill_manual(values = c("#FA7800", "#25CB10"), name="Race Context") +
    labs(title = "Race Context") +
    mapTheme() + theme(legend.position="bottom"), 
  ggplot() + geom_sf(data = na.omit(tracts20), aes(fill = incomeContext)) +
    scale_fill_manual(values = c("#25CB10", "#FA7800"), name="Income Context") +
    labs(title = "Income Context") +
    mapTheme() + theme(legend.position="bottom"))

#MAPE for both regressions by race
st_join(Philly_Data.test, tracts20) %>% 
  group_by(raceContext) %>%
  summarize(mean.MAPE = scales::percent(mean(sale_price.APE, na.rm = T))) %>%
  st_drop_geometry() %>%
  spread(raceContext, mean.MAPE) %>%
  kable(caption = "Test set MAPE by neighborhood racial context")

#MAPE for both regressions by income
st_join(Philly_Data.test, tracts20) %>% 
  filter(!is.na(incomeContext)) %>%
  group_by(incomeContext) %>%
  summarize(mean.MAPE = scales::percent(mean(sale_price.APE, na.rm = T))) %>%
  st_drop_geometry() %>%
  spread(incomeContext, mean.MAPE) %>%
  kable(caption = "Test set MAPE by neighborhood income context")

```

# Discussion
## Accuracy

## Generalizability

# Conclusion and Recommendation

