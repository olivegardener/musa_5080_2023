---
title: "PPA_Midterm"
author: "Oliver Atwood + Dave Drennan"
date: "2023-10-11"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    number_sections: yes
editor_options: 
  markdown: 
    wrap: 72
---

```{r packages, include=FALSE}
rm(list=ls())

knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"
source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

#some of these are repetitive with above - from Intro_to_ML_Pt`
library(ggplot2)
library(units)
library(httr)
library(rgdal)
library(corrplot)
library(tidyverse)
library(tidycensus)
library(dplyr)
library(sf)
library(spdep)
library(caret)
library(ckanr)
library(FNN)
library(grid)
library(gridExtra)
library(ggcorrplot) # plot correlation plot
library(corrr)      # another way to plot correlation plot
library(kableExtra)
library(jtools)     # for regression model plots
library(ggstance) # to support jtools plots
library(ggpubr)    # plotting R^2 value on ggplot point scatter
library(broom.mixed) # needed for effects plots
library(lubridate)
library(stargazer)
library(purrr)
library(tigris)
library(extrafont)
library(gtsummary)


# Coordinate System
coordinate_system <- 2272

options(scipen = 999)

Sys.setenv(OGR_GEOJSON_MAX_OBJ_SIZE = "1000000")  # Set to a large value

palette5 <- c("red", "pink", "grey",   "lightblue", "blue")
palette_sequential <- c("#ffffb2", "#fecc5c", "#fd8d3c", "#f03b20", "#bd0026")
palette_diverging <- c("#e66101", "#fdb863", "#f7f7f7", "#b2abd2", "#5e3c99")
palette_three <- c("#e66101", "#f7f7f7", "#5e3c99")

```
# Introduction



# Data

## Loading Data

```{r load data, results = "hide", fig.show='hide'}
# provided data

student_data <- st_read("data/studentData.geojson") %>% 
  st_transform(crs = coordinate_system) 

# wrangled data

philly_outline <- counties(state = "PA") %>% 
  filter(NAME == "Philadelphia") %>% 
  st_transform(crs = coordinate_system) %>% 
  select(NAME, geometry)

tracts2020 <- st_read("data/tracts2020.geojson") %>% 
  st_transform(crs = coordinate_system)

planning_districts <- st_read("data/Planning_Districts.geojson") %>% 
  st_transform(coordinate_system)

neighborhoods <- st_read("data/neighborhoods.geojson")

redlining <- st_read("data/redlining.geojson") %>% 
  st_transform(crs = coordinate_system) %>%
  mutate(
    holc_grade_val = case_when(
      holc_grade == "Commercial" ~ "0",
      holc_grade == "A" ~ "1",
      holc_grade == "B" ~ "2",
      holc_grade == "C" ~ "3",
      holc_grade == "D" ~ "4",
      )
  )

parks <- st_read("data/parks.geojson") %>% 
  st_transform(crs = coordinate_system)

vacant_lots <- st_read("data/Vacant_Lots.geojson") %>% 
  st_transform(crs = coordinate_system)

trees <- st_read("data/trees.geojson") %>%
  na.omit() %>%
  st_as_sf(coords = c("LOC_X", "LOC_Y"), crs = "EPSG:4326") %>%
  st_transform(crs = coordinate_system) %>% 
  dplyr::select(-TREE_NAME, -TREE_DBH, -YEAR)

septa <- st_read("data/septa.geojson") %>% 
  st_transform(crs = coordinate_system)
light_rail <- septa %>% 
  st_intersection(philly_outline, septa) %>%
  filter(
    (mode == "Highspeed" | mode == "Trolley"), 
    (directionn == "Northbound" | directionn == "Eastbound")
    )

food_access <- st_read("data/NeighborhoodFoodRetail.geojson") %>% 
  st_transform(crs = coordinate_system)

regional_rail <- st_read("data/Regional_Rail_Stations.geojson") %>% 
  st_transform(crs = coordinate_system) %>% 
  st_intersection(philly_outline, regional_rail)

rail_stops <- st_join(light_rail, regional_rail)

```

### Feature Engineering

```{r feature engineering}

# extract polygon data to points

model_data <- student_data %>% 
  dplyr::select(musaID, toPredict, sale_price, exterior_condition, fireplaces, interior_condition, number_of_bathrooms, number_of_bedrooms,
                  total_area, total_livable_area, year_built, geometry, ) %>% 
  mutate(age = 2023-year_built) %>% 
  dplyr::select(-year_built)

model_data <- model_data %>% 
          st_join(dplyr::select(redlining, holc_grade_val)) %>%
          st_join(dplyr::select(food_access, TOTAL_HPSS, TOTAL_LPSS)) %>% #HPSS is high produce supply store, LPSS is low produce
          st_join(dplyr::select(tracts2020, MedHHInc, PctWhite, PctBach)) %>%
          rename(
                 MedHHInc_ACS = MedHHInc,
                 PctWhite_ACS = PctWhite,
                 PctBach_ACS = PctBach,
                 quality_produce_access = TOTAL_HPSS,
                 low_produce_access = TOTAL_LPSS)%>% 
          mutate(PctWhite_ACS = PctWhite_ACS*100,
                 PctBach_ACS = PctBach_ACS*100,
                 holc_grade_val = as.numeric(holc_grade_val)
                 )

# min distance to parks

distances <- st_distance(model_data, parks)
min_distances <- apply(distances, 1, min)
model_data$dist2park <- min_distances

# K nearest neighbors

model_data <- model_data %>%
    mutate(
      rail_stop_nn1 = nn_function(st_coordinates(model_data), 
                                     st_coordinates(st_centroid(rail_stops)), 
                                     k = 1),
      vacant_lots_nn3 = nn_function(st_coordinates(model_data), 
                                    st_coordinates(st_centroid(vacant_lots)), 
                                    k = 3),
      )

# tree count buffers

model_data$tree.count.buffer <- model_data %>% 
    st_buffer(500) %>% 
    aggregate(mutate(trees, counter = 1),., sum) %>%
    pull(counter)

# imputing missing values

model_data <- model_data %>%
  mutate(
    exterior_condition = ifelse(is.na(exterior_condition), median(exterior_condition, na.rm = TRUE), exterior_condition),
    interior_condition = ifelse(is.na(interior_condition), median(interior_condition, na.rm = TRUE), interior_condition),
    number_of_bathrooms = ifelse(is.na(number_of_bathrooms), median(number_of_bathrooms, na.rm = TRUE), number_of_bathrooms),
    number_of_bedrooms = ifelse(is.na(number_of_bedrooms), median(number_of_bedrooms, na.rm = TRUE), number_of_bedrooms),
    fireplaces = ifelse(is.na(fireplaces), median(fireplaces, na.rm = TRUE), fireplaces),
    quality_produce_access = ifelse(is.na(quality_produce_access), median(quality_produce_access, na.rm = TRUE), quality_produce_access),
    low_produce_access = ifelse(is.na(low_produce_access), median(low_produce_access, na.rm = TRUE), low_produce_access),
    MedHHInc_ACS = ifelse(is.na(MedHHInc_ACS), median(MedHHInc_ACS, na.rm = TRUE), MedHHInc_ACS),
    PctWhite_ACS = ifelse(is.na(PctWhite_ACS), median(PctWhite_ACS, na.rm = TRUE), PctWhite_ACS),
    total_area = ifelse(is.na(total_area), median(total_area, na.rm = TRUE), total_area),
    tree.count.buffer = ifelse(is.na(tree.count.buffer), median(tree.count.buffer, na.rm = TRUE), tree.count.buffer),
    holc_grade_val = ifelse(is.na(holc_grade_val), median(holc_grade_val, na.rm = TRUE), holc_grade_val),
    age = ifelse(is.na(age), median(age, na.rm = TRUE), age),
    age = ifelse(age<0, 0, age),
    exterior_condition = as.integer(exterior_condition),
    number_of_bathrooms = as.integer(number_of_bathrooms),
    holc_grade_val = as.integer(holc_grade_val),
    tree.count.buffer = as.integer(tree.count.buffer)) %>%
    filter(sale_price < quantile(sale_price, 0.99))


model_data <- model_data %>%
  st_join(dplyr::select(neighborhoods, NAME))  %>%
  rename(neighborhood = NAME) %>%
  mutate(neighborhood = as.factor(neighborhood))


```

## Summary Statistics

```{r stargazer table}

# Convert sf object to data frame
model_data_df <- as.data.frame(model_data) %>%
  select(-musaID)

parameters <- names(model_data_df)

# option parameter can be used to set order based on index
stargazer(model_data_df, type = "text", title = "Table 1: Summary Statistics")

```

## Correlation matrix
```{r correlation matrix, fig.width=15, fig.height=15}
# Select variables of interest and convert them to numeric
vars_of_interest <- select_if(st_drop_geometry(model_data), is.numeric) %>% 
  select(-musaID) %>% 
  na.omit()

ggcorrplot(
  round(cor(vars_of_interest), 2), 
  p.mat = cor_pmat(vars_of_interest),
  colors = palette_three,
  type="lower",
  insig = "blank",
  lab = TRUE,  # Ensure labels are shown
  lab_size = 2.5  # Adjust the size of the labels
  ) +  
    labs(title = "Correlation across numeric variables") +
  theme(axis.text = element_text(size = 6)) 

```


## Home Price Correlation Scatterplots

```{r scatterplots, fig.width=10, fig.height=10}

st_drop_geometry(model_data) %>% 
  select(sale_price, vacant_lots_nn3, dist2park, quality_produce_access, tree.count.buffer) %>%
  gather(Variable, Value, -sale_price) %>% 
   ggplot(aes(Value, sale_price)) +
     geom_point(size = .01) + geom_smooth(method = "lm", se=F, colour = "#FA7800") +
     facet_wrap(~Variable, ncol = 2, scales = "free") +
     labs(title = "Price as a function of continuous variables") +
     plotTheme()

```

These scatterplots show the expected correlations for each of these variables. There is a slight negative correlation between distance to park and sale price, meaning that generally speaking, the closer a house is to a park, the higher its sale price. For tree.count.buffer, vacant_lots_nn3, and quality_produce_access, these scatterplots show a positive correlation with sale_price, meaning that in general the more trees near a given house, the farther it is from the nearest three vacant lots, and the closer it is to  quality produce, the higher its expected sale price.

## Map of Home Prices

```{r, fig.width=15}
salepricemap <- ggplot() +
  geom_sf(data = neighborhoods, fill = "white") +
  geom_sf(data = model_data, 
          aes(colour = sale_price), 
          show.legend = "point", size = .001) +
  scale_colour_gradientn(colors = palette_sequential,
                         name="Sale Price",
                         labels = scales::label_number(scale = 1),
                         guide = guide_colourbar(title.position = "top", title.hjust = 0.5)) +
  labs(title="Sale Price, Philly") +
  mapTheme()

salepricemap

```

## Maps of Independent Variables

### Tree Density
```{r map1, fig.width=15}

ggplot() + geom_sf(data = neighborhoods, fill = "white", color = "grey80") +
  stat_density2d(data = data.frame(st_coordinates(trees)), 
                 aes(X, Y, fill = ..level.., alpha = ..level..),
                 size = 0.01, bins = 40, geom = 'polygon') +
  scale_fill_gradient(low = "lightgreen", high = "darkgreen",
                      name = "Density") +
  scale_alpha(range = c(0.00, 0.35), guide = FALSE) +
  labs(title = "Density of Tree Locations") +
  mapTheme()

```
This density plot of trees appears to line up with the higher sale price areas, particularly the one in downtown.

### High Quality Produce Access
```{r map2, fig.width=15}
ggplot() + geom_sf(data = food_access, aes(fill = as.numeric(TOTAL_HPSS)), color = "grey80") +
  scale_fill_gradient(low = "lavender", high = "purple", name = "Store Count")+
  labs(title = "High Quality Produce Access by Store Count",
       subtitle = "Data provided per block group") +
  mapTheme()

```

### Vacant Lot Density
```{r map3, fig.width=15}
# Extracting centroids and converting to data.frame
centroids <- st_centroid(vacant_lots)
centroids_df <- data.frame(st_coordinates(centroids))

ggplot() + geom_sf(data = neighborhoods, fill = "white", color = "grey80") +
  stat_density2d(data = centroids_df, 
                 aes(X, Y, fill = ..level.., alpha = ..level..),
                 size = 0.01, bins = 40, geom = 'polygon') +
  scale_fill_gradient(low = "pink", high = "red",
                      name = "Density") +
  scale_alpha(range = c(0.00, 0.35), guide = FALSE) +
  labs(title = "Density of Vacant Lot Locations") +
  mapTheme()
```


This appears to correlate spatially with the more expensive homes in the northeast and northwest of the city, but given the high levels of vacant lots in center city, which coincide with high housing prices, this may not be the best predictor of housing price.

# Methods


# Results

## Table of Results (Training Data)
```{r regression partition and summary results}

Philly_Data <- model_data %>%
  filter(toPredict == "MODELLING",
         age < 500) %>%
  dplyr::select(-toPredict,)

# partition
inTrain <- createDataPartition(
              y = paste(Philly_Data$neighborhood),
              p = .70, list = FALSE)
Philly_Data.training <- st_drop_geometry(Philly_Data[inTrain,]) %>%
  dplyr::select(-musaID)
Philly_Data.test <- Philly_Data[-inTrain,] %>%
  dplyr::select(-musaID) 
 
reg.training <- 
  lm(sale_price ~ ., data = as.data.frame(Philly_Data.training)) 

reg.training %>%
  tbl_regression(intercept = TRUE) %>%
  modify_column_hide(column = ci) %>%
  modify_column_unhide(column = std.error) %>%
  kbl() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
  scroll_box(width = "100%", height = "200px",
             fixed_thead = FALSE)
```

## Table of Goodness of Fit (Test Data)
```{r goodness of fit, results = "hide"}

Philly_Data.test <-
  Philly_Data.test %>%
  mutate(sale_price.Predict = predict(reg.training, Philly_Data.test),
         sale_price.Error = sale_price.Predict - sale_price,
         sale_price.AbsError = abs(sale_price.Predict - sale_price),
         sale_price.APE = abs((sale_price.Predict - sale_price) / sale_price.Predict))
 
# st_drop_geometry(Philly_Data.test) %>%
#   select(sale_price.Predict, sale_price.Error, sale_price.AbsError, sale_price.APE) %>%
#   kbl()

st_drop_geometry(Philly_Data.test) %>%
  gather(Variable, Value, -neighborhood) %>%
  filter(Variable == "sale_price.AbsError" | Variable == "sale_price.APE") %>%
  group_by(Variable) %>%
    summarize(meanValue = mean(Value, na.rm = T)) %>%
    spread(Variable, meanValue) %>% kbl(col.names = c('Sale Price Mean Absolute Error', 'Sale Price APE'), digits = 3, format.args = list(big.mark = ",")) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

```


## Cross Validation Results

```{r cross validation}

fitControl <- trainControl(method = "cv", number = 100)
set.seed(825)

reg.cv <- 
  train(sale_price ~ ., data = st_drop_geometry(Philly_Data.training),
     method = "lm", trControl = fitControl, na.action = na.pass)

reg.cv

plot1 <- reg.cv$resample

ggplot()+
  geom_histogram(data=plot1, aes(x = MAE))

```

## Predicted vs Observed Scatterplot

```{r pred and obs plot}

Philly_Data.test %>%
  dplyr::select(sale_price.Predict, sale_price) %>%
    ggplot(aes(sale_price, sale_price.Predict)) +
  geom_point() +
  stat_smooth(aes(sale_price.Predict, sale_price), 
              method = "lm", se = FALSE, size = 1, colour="#25CB10") +
  stat_smooth(aes(sale_price, sale_price), 
             method = "lm", se = FALSE, size = 1, colour="#FA7800") + 
  labs(title="Predicted sale price as a function of observed price",
       subtitle="Orange line represents a perfect prediction; Green line represents prediction") +
  plotTheme()

```

## Residuals and Moran's I Test

### Residuals Map
```{r residuals plot, fig.width=15}

ggplot() +
  geom_sf(data = neighborhoods, fill = "white") +
  geom_sf(data = Philly_Data.test, aes(colour = q5(sale_price.Error)), 
          show.legend = "point", size = .1) +
  scale_colour_manual(values = palette5,
                   
                   name="Quintile\nBreaks") +
  labs(title="Residuals, Philly") +
  mapTheme()

```
  
### Spatial Lag in Errors
```{r spatial lag in errors map}

coords.test <-  st_coordinates(Philly_Data.test)

neighborList.test <- knn2nb(knearneigh(coords.test, 5))

spatialWeights.test <- nb2listw(neighborList.test, style="W")

Philly_Data.test %>%
  mutate(lagPriceError = lag.listw(spatialWeights.test, sale_price.Error)) %>%
  ggplot()+
  geom_point(aes(x =lagPriceError, y =sale_price.Error))
```

### Moran's I Plot
```{r moran}

moranTest <- moran.mc(Philly_Data.test$sale_price.Error,
                      spatialWeights.test, nsim = 999)


ggplot(as.data.frame(moranTest$res[c(1:999)]), aes(moranTest$res[c(1:999)])) +
  geom_histogram(binwidth = 0.01) +
  geom_vline(aes(xintercept = moranTest$statistic), colour = "#FA7800",size=1) +
  scale_x_continuous(limits = c(-1, 1)) +
  labs(title="Observed and permuted Moran's I",
       subtitle= "Observed Moran's I in orange",
       x="Moran's I",
       y="Count") +
  plotTheme()

```

## Predicted Sales Prices Map

```{r pred map, fig.width=15}

# Ensure that 'Philly_Data.challenge' only contains the observations you want to predict
Philly_Data.challenge <- model_data

# Generate predictions only for 'Philly_Data.challenge'
Philly_Data.challenge <- Philly_Data.challenge %>%
  mutate(sale_price_prediction = predict(reg.training, .))

# Determine global limits
global_min <- min(min(model_data$sale_price), min(Philly_Data.challenge$sale_price_prediction))
global_max <- max(max(model_data$sale_price), max(Philly_Data.challenge$sale_price_prediction))

# # ggplot salepricemap
# salepricemap <- ggplot() +
#   geom_sf(data = neighborhoods, fill = "white") +
#   geom_sf(data = model_data %>% filter(sale_price < quantile(sale_price, 0.99)), 
#           aes(colour = sale_price), 
#           show.legend = "point", size = .001) +
#   scale_colour_gradientn(colors = palette_sequential,
#                          name="Sale Price",
#                          labels = scales::label_number(scale = 1),
#                          guide = guide_colourbar(title.position = "top", title.hjust = 0.5),
#                          limits = c(global_min, global_max)) +  # Set limits
#   labs(title="Sale Price, Philly") +
#   mapTheme()

# ggplot predictedsalepricemap
predictedsalepricemap <- ggplot() +
  geom_sf(data = neighborhoods, fill = "white") +
  geom_sf(data = Philly_Data.challenge, aes(colour = sale_price_prediction),
          show.legend = "point", size = .01) +
  scale_colour_gradientn(colors = palette_sequential,
                         name="Predicted Sale Price",
                         labels = scales::label_number(scale = 1),
                         guide = guide_colourbar(title.position = "top", title.hjust = 0.5),
                         limits = c(global_min, global_max)) +  # Set limits
  labs(title="Predicted Sale Price, Philly") +
  mapTheme()

# Arrange the plots side by side
# grid.arrange(salepricemap, predictedsalepricemap, ncol=2)

predictedsalepricemap

# export sales price predictions for challenge data to csv
# Philly_Data.challenge.csv <- st_drop_geometry(Philly_Data.challenge) %>%
#   filter(toPredict == "CHALLENGE") %>%
#   dplyr::select(musaID, sale_price_prediction) %>%
#   mutate(teamName = "House It Going")
# 
# write.csv(Philly_Data.challenge.csv, "HouseItGoing.csv", row.names = FALSE)

```

## Mean Absolute Percent Error by Neighborhood Map

```{r mape map, fig.width=15}
# Spatial join
Philly_Data.test <- st_join(Philly_Data.test, neighborhoods, join = st_within)

# Grouping and summarization
mean_MAPE_by_neighborhood <- st_drop_geometry(Philly_Data.test) %>%
  group_by(neighborhood) %>%
  summarise(mean.MAPE = mean(sale_price.APE, na.rm = TRUE)) %>%
  ungroup()

neighborhoods <- neighborhoods %>%
  left_join(mean_MAPE_by_neighborhood, by = c("NAME" = "neighborhood"))


MAPE.Map <- ggplot() + 
  geom_sf(data = neighborhoods, aes(fill = mean.MAPE)) +
  scale_fill_gradient(low = "lightblue", high = "red",
                        name = "MAPE") +
  labs(title = "Mean test set MAPE by neighborhood") +
  mapTheme()

MAPE.Map

```

This shows that the model is good at predicting across much of the city, with errors clustered in one neighborhood. Why is the absolute percent error so high in that neighborhood? Perhaps there are a very low number of houses in our training data for that neighborhood, relative to others. This could be skewing our results. 
```{r}
HouseCount_by_neighborhood <- st_drop_geometry(Philly_Data.training) %>%
  group_by(neighborhood) %>%
  summarise(HouseCount = n()) %>% 
  ungroup()

neighborhoods <- neighborhoods %>%
  left_join(HouseCount_by_neighborhood, by = c("NAME" = "neighborhood"))


HouseCount.Map <- ggplot() + 
  geom_sf(data = neighborhoods, aes(fill = HouseCount)) +
  scale_fill_gradient(low = "red", high = "lightblue",
                        name = "House Count") +
  labs(title = "Number of Houses Per Neighborhood") +
  mapTheme()

HouseCount.Map
```
Here is a count of houses per neighborhood. We can see from this plot that one of the neighborhoods we are predicting poorly for has a low number of houses, but there are others that have closer to the average number of houses but are predicting poorly. There must be other factors causing our poor prediction in thse neighborhoods.

## Mean Absolute Percent Error by Neighborhood Mean Price Scatterplot
```{r mape scatterplot, fig.width = 10}
# Calculate mean price by neighborhood
mean_price_by_neighborhood <- st_drop_geometry(Philly_Data.test) %>%
  group_by(neighborhood) %>%
  summarise(mean_price = mean(sale_price, na.rm = TRUE)) %>%
  ungroup()

# Merge mean_price_by_neighborhood and mean_MAPE_by_neighborhood
neighborhood_summary <- left_join(mean_price_by_neighborhood, mean_MAPE_by_neighborhood, by = "neighborhood")

# Create a scatterplot of MAPE by neighborhood as a function of mean price by neighborhood
scatterplot <- ggplot(neighborhood_summary, aes(x = mean_price, y = mean.MAPE)) +
  geom_point(aes(color = mean.MAPE), size = 1) +
  scale_color_gradient(low = "lightgreen", high = "red") +
  labs(
    title = "Scatterplot of MAPE by Neighborhood as a Function of Mean Price",
    x = "Mean Price by Neighborhood",
    y = "Mean MAPE by Neighborhood",
    color = "MAPE"
  ) +
  theme_minimal()

# Display the scatterplot
print(scatterplot)


```


## Model Generalizability by Demographics and Income

```{r tracts, fig.width=10, results = "hide", fig.show='hide'}
tracts20 <- 
  get_acs(geography = "tract", variables = c("B01001_001E","B01001A_001E","B06011_001"), 
          year = 2020, state="PA", county="Philadelphia", geometry=T, output = "wide") %>%
  st_transform(crs=2272)  %>%
  rename(TotalPop = B01001_001E,
         NumberWhites = B01001A_001E,
         Median_Income = B06011_001E) %>%
  mutate(percentWhite = NumberWhites / TotalPop,
         raceContext = ifelse(percentWhite > .5, "Majority White", "Majority Non-White"),
         incomeContext = ifelse(Median_Income > 32322, "High Income", "Low Income"))

grid.arrange(ncol = 2,
  ggplot() + geom_sf(data = na.omit(tracts20), aes(fill = raceContext)) +
    scale_fill_manual(values = c("#FA7800", "#25CB10"), name="Race Context") +
    labs(title = "Race Context") +
    mapTheme() + theme(legend.position="bottom"), 
  ggplot() + geom_sf(data = na.omit(tracts20), aes(fill = incomeContext)) +
    scale_fill_manual(values = c("#25CB10", "#FA7800"), name="Income Context") +
    labs(title = "Income Context") +
    mapTheme() + theme(legend.position="bottom"))

#MAPE for both regressions by race
st_join(Philly_Data.test, tracts20) %>% 
  group_by(raceContext) %>%
  summarize(mean.MAPE = scales::percent(mean(sale_price.APE, na.rm = T))) %>%
  st_drop_geometry() %>%
  spread(raceContext, mean.MAPE) %>%
  kable(caption = "Test set MAPE by neighborhood racial context")

#MAPE for both regressions by income
st_join(Philly_Data.test, tracts20) %>% 
  filter(!is.na(incomeContext)) %>%
  group_by(incomeContext) %>%
  summarize(mean.MAPE = scales::percent(mean(sale_price.APE, na.rm = T))) %>%
  st_drop_geometry() %>%
  spread(incomeContext, mean.MAPE) %>%
  kable(caption = "Test set MAPE by neighborhood income context")

```
From these maps, it appears that our model performs with lower accuracy in neighborhoods with larger proportions of residents of color and lower income residents. Why?


# Discussion
## Accuracy

## Generalizability

# Conclusion and Recommendation

