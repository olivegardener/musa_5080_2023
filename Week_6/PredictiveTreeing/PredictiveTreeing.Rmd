---
title: "Predictive Tree Work"
author: "Oliver Atwood"
date: "2023-10-17"
output: html_document
---
Your job is to build a version of the spatial risk model seen in the book (and in the Week 6 "Predictive Policing" lab) for a different outcome that likely suffers from more selection bias than burglary. You can build this model in Chicago or any other city with sufficient open data resources.

Please also add at least two new independent features not used in Chapter 5, and iteratively build models until you have landed on one that optimizes for accuracy and generalizability.

Your final deliverable should be in R markdown form with code (please hide the code blocks). Please provide the following materials with brief annotations (please don’t forget this):

A map of your outcome of interest in point form, with some description of what, when, and why you think selection bias may be an issue.
A map of your outcome joined to the fishnet.
A small multiple map of your risk factors in the fishnet (counts, distance and/or other feature engineering approaches).
Local Moran’s I-related small multiple map of your outcome (see 5.4.1)
A small multiple scatterplot with correlations.
A histogram of your dependent variable.
A small multiple map of model errors by random k-fold and spatial cross validation.
A table of MAE and standard deviation MAE by regression.
A table of raw errors by race context for a random k-fold vs. spatial cross validation regression.
The map comparing kernel density to risk predictions for the next year’s crime.
The bar plot making this comparison.
Two paragraphs on why or why not you would recommend your algorithm be put into production.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())

# root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"
source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

```

```{r}
library(tidyverse)
library(sf)
library(RSocrata)
library(viridis)
library(spatstat)
library(raster)
library(spdep)
library(FNN)
library(grid)
library(gridExtra)
library(knitr)
library(kableExtra)
library(tidycensus)
library(classInt)   # for KDE and ML risk class intervals
library(tigris)
library(arcpullr)
library(units)

```

## R Markdown

```{r cars}
# Coordinate System
coordinate_system <- 2272

#Load Boundary File
philly_boundary <- counties(state = 42) %>% 
  filter(NAME == "Philadelphia") %>% 
  st_transform(crs = coordinate_system)

```
# 311 Requests Data Wrangling
```{r}
# 311 Requests API (needs work)
# StreetTreeRequests_ALL <- get_spatial_layer("https://services.arcgis.com/fLeGjb7u4uXqeF9q/ArcGIS/rest/services/philly311__public_cases/FeatureServer/0/") %>%
#   filter(service_name == 'Street Trees') %>%
#   filter(lat != "") %>%
#   st_as_sf(coords = c("lon", "lat"), crs = 4326) %>%
#   st_transform(crs = coordinate_system)
  
# 311 Requests (by year)
# StreetTreeRequests <- st_read("/Users/oliveratwood/Downloads/Philly 311/public_cases_fc2022.csv") %>%
# StreetTreeRequests <- st_read("/Users/oliveratwood/Downloads/Philly 311/public_cases_fc2021.csv") %>%
StreetTreeRequests <- st_read("/Users/oliveratwood/Downloads/Philly 311/public_cases_fc2019.csv") %>%
# StreetTreeRequests <- st_read("/Users/oliveratwood/Downloads/Philly 311/public_cases_fc2018.csv") %>%
  filter(service_name == 'Street Trees') %>%
  filter(lat != "") %>%
  st_as_sf(coords = c("lon", "lat"), crs = 4326) %>%
  st_transform(crs = coordinate_system)

# Specify the file path where you want to save the GeoJSON file
# file_path <- "/Users/oliveratwood/Documents/GitHub/musa_5080_2023/Week_6/PredictiveTreeing/Data/StreetTreeRequests2022.geojson"
# file_path <- "/Users/oliveratwood/Documents/GitHub/musa_5080_2023/Week_6/PredictiveTreeing/Data/StreetTreeRequests2021.geojson"
file_path <- "/Users/oliveratwood/Documents/GitHub/musa_5080_2023/Week_6/PredictiveTreeing/Data/StreetTreeRequests2019.geojson"
# file_path <- "/Users/oliveratwood/Documents/GitHub/musa_5080_2023/Week_6/PredictiveTreeing/Data/StreetTreeRequests2018.geojson"

# Write the data frame as GeoJSON
st_write(StreetTreeRequests, file_path)
```

# Load in Selected 311 Data
```{r}
# !!!!!!link to github raw!
# Load Street Tree 311 Requests (previously cleaned, see commented code above)
StreetTreeRequests <- st_read("https://raw.githubusercontent.com/olivegardener/musa_5080_2023/main/Week_6/PredictiveTreeing/Data/StreetTreeRequests2018.geojson")


Week_6/PredictiveTreeing/Data/StreetTreeRequests.geojson
```


```{r fig.width=6, fig.height=4}
# uses grid.arrange to organize independent plots
grid.arrange(ncol=2,
ggplot() + 
  geom_sf(data = philly_boundary) +
  geom_sf(data = StreetTreeRequests, colour="darkgreen", size=0.1, show.legend = "point") +
  labs(title= "Tree 311 Calls, Philadelphia - 2022") +
  mapTheme(title_size = 10),

ggplot() + 
  geom_sf(data = philly_boundary, fill = "grey40") +
  stat_density2d(data = data.frame(st_coordinates(StreetTreeRequests)), 
                 aes(X, Y, fill = ..level.., alpha = ..level..),
                 size = 0.01, bins = 40, geom = 'polygon') +
  scale_fill_viridis() +
  scale_alpha(range = c(0.00, 0.35), guide = FALSE) +
  labs(title = "Density of Tree 311 Calls") +
  mapTheme(title_size = 10) + theme(legend.position = "none"))
```

```{r}
## using {sf} to create the grid
## Note the `.[philly_boundary] %>% ` line. This is needed to clip the grid to our data
fishnet <- 
  st_make_grid(philly_boundary,
               cellsize = 5000, 
               square = TRUE) %>%
               # hexagon = TRUE) %>%
  .[philly_boundary] %>%            # fast way to select intersecting polygons
  st_sf() %>%
  mutate(uniqueID = 1:n())

```

```{r}
## add a value of 1 to each tree request, sum them with aggregate
tree_net <- 
  dplyr::select(StreetTreeRequests) %>% 
  mutate(countRequests = 1) %>% 
  aggregate(., fishnet, sum) %>%
  mutate(countRequests = replace_na(countRequests, 0),
         uniqueID = 1:n(),
         cvID = sample(round(nrow(fishnet) / 24), 
                       size=nrow(fishnet), replace = TRUE))

ggplot() +
  geom_sf(data = tree_net, aes(fill = countRequests), color = NA) +
  scale_fill_viridis() +
  labs(title = "Count of Street Tree Service Requests for the Fishnet") +
  mapTheme()
```

## Modeling Spatial Features
```{r}
trees <- get_spatial_layer("https://services.arcgis.com/fLeGjb7u4uXqeF9q/arcgis/rest/services/PPR_Tree_Inventory_2022/FeatureServer/0/") %>%
  filter(TREE_DBH > 0, TREE_DBH < 100) %>% 
  st_transform(crs = coordinate_system) %>% 
  st_join(philly_boundary) %>%
  mutate(Legend = "PhillyTrees") %>%
  mutate(Legend2 = "DBH")


#2020 ACS data for Philly
tracts2020 <- 
  get_acs(geography = "tract", 
          variables = c("B25026_001E",
                        "B19013_001E",
                        "DP04_0046PE"), 
          year=2020, state=42, county="101", 
          geometry=TRUE, output="wide") %>%
  st_transform(coordinate_system) %>%
  rename(TotalPop = B25026_001E, 
         MedHHInc = B19013_001E, 
         HomeOwnRate = DP04_0046PE) %>% 
  mutate(area = st_area(geometry)) %>%
  mutate(year = "2020", 
         PopDensity = drop_units(TotalPop / (area / 2.788e+7))) %>% 
  dplyr::select(-NAME, -TotalPop, -area, -starts_with("D"), -starts_with("B"))

neighborhoods <- get_spatial_layer("https://services1.arcgis.com/a6oRSxEw6eIY5Zfb/ArcGIS/rest/services/Philadelphia_Neighborhoods/FeatureServer/0")%>%
  st_transform(crs = 2272)
community_landcare <- get_spatial_layer("https://services.arcgis.com/fLeGjb7u4uXqeF9q/arcgis/rest/services/PHS_CommunityLandcare/FeatureServer/0/") %>% 
  st_transform(crs = 2272)
landcare_maintenance <- get_spatial_layer("https://services.arcgis.com/fLeGjb7u4uXqeF9q/arcgis/rest/services/PHS_PhilaLandCare_Maintenance/FeatureServer/0/") %>%
  st_transform(crs = 2272)

community_landcare <- st_centroid(community_landcare)
community_landcare <- st_geometry(community_landcare)
community_landcare <- st_sf(geometry = community_landcare)

landcare_maintenance <- st_centroid(landcare_maintenance)
landcare_maintenance <- st_geometry(landcare_maintenance)
landcare_maintenance <- st_sf(geometry = landcare_maintenance)

landcare_lots <- rbind(community_landcare, landcare_maintenance)


```

```{r}
vars_net <- trees %>%
  st_join(fishnet, join=st_within) %>%
  st_drop_geometry() %>%
  group_by(uniqueID, Legend) %>%
  summarize(count = n(), 
            sum_TREE_DBH = sum(TREE_DBH, na.rm = TRUE)) %>% # Calculate sum of TREE_DBH
  left_join(fishnet, ., by = "uniqueID") %>%
  spread(Legend, count, fill=0) %>% 
  dplyr::select(-`<NA>`) %>%
  ungroup() %>% 
  mutate(sum_TREE_DBH = ifelse(is.na(sum_TREE_DBH), 0, sum_TREE_DBH))  # Replace NA values with 0

```

## Nearest Neighbor Feature

> Review: what is NN and what does `k` represent in this function?

```{r}
# convinience to reduce length of function names.
st_c    <- st_coordinates
st_coid <- st_centroid

## create NN from abandoned cars
vars_net <- vars_net %>%
    mutate(trees.nn = nn_function(st_c(st_coid(vars_net)), 
                                           st_c(trees),
                                           k = 5))
```
## Join NN feature to our fishnet

Since the counts were aggregated to each cell by `uniqueID` we can use that to join the counts to the fishnet.

```{r}
## important to drop the geometry from joining features
final_net <-
  left_join(tree_net, st_drop_geometry(vars_net), by="uniqueID") 

```

### Join in areal data

Using spatial joins to join *centroids* of fishnets to polygon for neighborhoods and districts.

> What issues arise when we try to join polygons to polygons in space?

```{r}

final_net <-
  st_centroid(final_net) %>%
    st_join(dplyr::select(tracts2020, MedHHInc), by = "uniqueID") %>%
    st_join(dplyr::select(tracts2020, HomeOwnRate), by = "uniqueID") %>%
    st_join(dplyr::select(tracts2020, PopDensity), by = "uniqueID") %>%
    st_join(dplyr::select(neighborhoods, NAME), by = "uniqueID") %>%
      st_drop_geometry() %>%
      left_join(dplyr::select(final_net, geometry, uniqueID)) %>%
      st_sf() %>%
  na.omit()

# for live demo
# mapview::mapview(final_net, zcol = "District")
```

## Local Moran's I for fishnet grid cells

using {spdep} package to to build neighborhood weights and list to calculate local Moran's I.

Note that the code here is *different* than in the book - it has been updated to keep up with changes in packages.

> What is the difference between local and global Moran's I?

A little in depth version of the chunk below can be found:

Mendez C. (2020). Spatial autocorrelation analysis in R. R Studio/RPubs. Available at <https://rpubs.com/quarcs-lab/spatial-autocorrelation>

```{r}
## generates warnings from PROJ issues
## {spdep} to make polygon to neighborhoods... 
final_net.nb <- poly2nb(as_Spatial(final_net), queen=TRUE)
## ... and neighborhoods to list of weigths
final_net.weights <- nb2listw(final_net.nb, style="W", zero.policy=TRUE)

# print(final_net.weights, zero.policy=TRUE)
```

```{r}
## see ?localmoran
local_morans <- localmoran(final_net$countRequests, final_net.weights, zero.policy=TRUE) %>% 
  as.data.frame()

# join local Moran's I results to fishnet
final_net.localMorans <- 
  cbind(local_morans, as.data.frame(final_net)) %>% 
  st_sf() %>%
  dplyr::select(TreeCount = PhillyTrees, 
                Local_Morans_I = Ii, 
                P_Value = `Pr(z != E(Ii))`) %>%
  mutate(Significant_Hotspots = ifelse(P_Value <= 0.001, 1, 0)) %>%
  gather(Variable, Value, -geometry)
  
```
### Plotting local Moran's I results

This is a complex code chunk - it's a loop which builds ggplots of local Moran's for each of your `vars`

> What does a significant hot spot tell us about the distribution of tree requests?

```{r}
## This is just for plotting
vars <- unique(final_net.localMorans$Variable)
varList <- list()

for(i in vars){
  varList[[i]] <- 
    ggplot() +
      geom_sf(data = filter(final_net.localMorans, Variable == i), 
              aes(fill = Value), colour=NA) +
      scale_fill_viridis(name="") +
      labs(title=i) +
      mapTheme(title_size = 14) + theme(legend.position="bottom")}

do.call(grid.arrange,c(varList, ncol = 4, top = "Local Morans I statistics, Street Tree 311 Requests"))
```

## Distance to Hot spot

Using NN distance to a hot spot location

```{r}
# generates warning from NN
final_net <- final_net %>% 
  mutate(Trees.isSig = 
           ifelse(local_morans[,5] <= 0.001, 1, 0)) %>%
  mutate(Trees.isSig.dist = 
           nn_function(st_c(st_coid(final_net)),
                       st_c(st_coid(filter(final_net, 
                                           Trees.isSig == 1))), 
                       k = 1))

## What does k = 1 represent?
```

> What does `k = 1` above mean in terms of measuring nearest neighbors?

### Plot NN distance to hot spot

```{r}
ggplot() +
      geom_sf(data = final_net, aes(fill=Trees.isSig.dist), colour=NA) +
      scale_fill_viridis(name="NN Distance") +
      labs(title="Street Tree 311 Requests NN Distance") +
      mapTheme()
```

## Modeling and CV

Leave One Group Out CV on spatial features

```{r sults='hide'}

## define the variables we want
reg.ss.vars <- c("trees.nn", "Trees.isSig.dist")

# reg.ss.vars <- c("sum_TREE_DBH", "MedHHInc", "HomeOwnRate", "PopDensity", "Trees.isSig.dist")


## RUN REGRESSIONS
reg.ss.spatialCV <- crossValidate(
  dataset = final_net,
  id = "uniqueID",                           
  dependentVariable = "countRequests",
  indVariables = reg.ss.vars) %>%
    dplyr::select(cvID = uniqueID, countRequests, Prediction, geometry)
```

```{r}
# calculate errors by NEIGHBORHOOD
error_by_reg_and_fold <- 
  reg.ss.spatialCV %>%
    group_by(cvID) %>% 
    summarize(Mean_Error = mean(Prediction - countRequests, na.rm = T),
              MAE = mean(abs(Mean_Error), na.rm = T),
              SD_MAE = mean(abs(Mean_Error), na.rm = T)) %>%
  ungroup()

error_by_reg_and_fold %>% 
  arrange(desc(MAE))
error_by_reg_and_fold %>% 
  arrange(MAE)

## plot histogram of OOF (out of fold) errors
error_by_reg_and_fold %>%
  ggplot(aes(MAE)) + 
    geom_histogram(bins = 30, colour="black", fill = "#FDE725FF") +
  scale_x_continuous(breaks = seq(0, 11, by = 1)) + 
    labs(title="Distribution of MAE", subtitle = "LOGO-CV",
         x="Mean Absolute Error", y="Count") 
```

## Density vs predictions

The `spatstat` function gets us kernal density estimates with varying search radii.

Note that the code here is *different* than in the book - it has been updated to keep up with changes in packages.

```{r}
# demo of kernel width
burg_ppp <- as.ppp(st_coordinates(StreetTreeRequests), W = st_bbox(final_net))
burg_KD.1000 <- density.ppp(burg_ppp, 1000)
burg_KD.1500 <- density.ppp(burg_ppp, 1500)
burg_KD.2000 <- density.ppp(burg_ppp, 2000)
burg_KD.df <- rbind(
  mutate(data.frame(rasterToPoints(mask(raster(burg_KD.1000), as(neighborhoods, 'Spatial')))), Legend = "1000 Ft."),
  mutate(data.frame(rasterToPoints(mask(raster(burg_KD.1500), as(neighborhoods, 'Spatial')))), Legend = "1500 Ft."),
  mutate(data.frame(rasterToPoints(mask(raster(burg_KD.2000), as(neighborhoods, 'Spatial')))), Legend = "2000 Ft.")) 

burg_KD.df$Legend <- factor(burg_KD.df$Legend, levels = c("1000 Ft.", "1500 Ft.", "2000 Ft."))

ggplot(data=burg_KD.df, aes(x=x, y=y)) +
  geom_raster(aes(fill=layer)) + 
  facet_wrap(~Legend) +
  coord_sf(crs=st_crs(final_net)) + 
  scale_fill_viridis(name="Density") +
  labs(title = "Kernel density with 3 different search radii") +
  mapTheme(title_size = 14)
```

```{r}

as.data.frame(burg_KD.1000) %>%
  st_as_sf(coords = c("x", "y"), crs = st_crs(final_net)) %>%
  aggregate(., final_net, mean) %>%
   ggplot() +
     geom_sf(aes(fill=value)) +
     geom_sf(data = sample_n(StreetTreeRequests, 1500), color = 'orange',size = .01) +
     scale_fill_viridis(name = "Density") +
     labs(title = "Kernel density of 2022 Street Tree 311 Requests") +
     mapTheme(title_size = 14)
```

## Get 2018 crime data

Let's see how our model performed relative to KD on the following year's data.

```{r}

StreetTreeRequests19 <- st_read("https://raw.githubusercontent.com/olivegardener/musa_5080_2023/main/Week_6/PredictiveTreeing/Data/StreetTreeRequests2019.geojson") %>%
  .[fishnet,]
```

```{r}

burg_KDE_sum <- as.data.frame(burg_KD.1000) %>%
  st_as_sf(coords = c("x", "y"), crs = st_crs(final_net)) %>%
  aggregate(., final_net, mean) 
kde_breaks <- classIntervals(burg_KDE_sum$value, 
                             n = 5, "fisher")
burg_KDE_sf <- burg_KDE_sum %>%
  mutate(label = "Kernel Density",
         Risk_Category = classInt::findCols(kde_breaks),
         Risk_Category = case_when(
           Risk_Category == 5 ~ "5th",
           Risk_Category == 4 ~ "4th",
           Risk_Category == 3 ~ "3rd",
           Risk_Category == 2 ~ "2nd",
           Risk_Category == 1 ~ "1st")) %>%
  cbind(
    aggregate(
      dplyr::select(burglaries18) %>% mutate(burgCount = 1), ., sum) %>%
    mutate(burgCount = replace_na(burgCount, 0))) %>%
  dplyr::select(label, Risk_Category, burgCount)
```

Note that this is different from the book, where we pull a model out of a list of models we've created. For your homework, you'll be creating multiple models.

```{r}
ml_breaks <- classIntervals(reg.ss.spatialCV$Prediction, 
                             n = 5, "fisher")
burg_risk_sf <-
  reg.ss.spatialCV %>%
  mutate(label = "Risk Predictions",
         Risk_Category =classInt::findCols(ml_breaks),
         Risk_Category = case_when(
           Risk_Category == 5 ~ "5th",
           Risk_Category == 4 ~ "4th",
           Risk_Category == 3 ~ "3rd",
           Risk_Category == 2 ~ "2nd",
           Risk_Category == 1 ~ "1st")) %>%
  cbind(
    aggregate(
      dplyr::select(burglaries18) %>% mutate(burgCount = 1), ., sum) %>%
      mutate(burgCount = replace_na(burgCount, 0))) %>%
  dplyr::select(label,Risk_Category, burgCount)
```

We don't do quite as well because we don't have very many features, but still pretty good.

```{r}
rbind(burg_KDE_sf, burg_risk_sf) %>%
  na.omit() %>%
  gather(Variable, Value, -label, -Risk_Category, -geometry) %>%
  ggplot() +
    geom_sf(aes(fill = Risk_Category), colour = NA) +
    geom_sf(data = sample_n(burglaries18, 3000), size = .5, colour = "black") +
    facet_wrap(~label, ) +
    scale_fill_viridis(discrete = TRUE) +
    labs(title="Comparison of Kernel Density and Risk Predictions",
         subtitle="2017 burglar risk predictions; 2018 burglaries") +
    mapTheme(title_size = 14)
```

```{r}
rbind(burg_KDE_sf, burg_risk_sf) %>%
  st_drop_geometry() %>%
  na.omit() %>%
  gather(Variable, Value, -label, -Risk_Category) %>%
  group_by(label, Risk_Category) %>%
  summarize(countBurglaries = sum(Value)) %>%
  ungroup() %>%
  group_by(label) %>%
  mutate(Pcnt_of_test_set_crimes = countBurglaries / sum(countBurglaries)) %>%
    ggplot(aes(Risk_Category,Pcnt_of_test_set_crimes)) +
      geom_bar(aes(fill=label), position="dodge", stat="identity") +
      scale_fill_viridis(discrete = TRUE, name = "Model") +
      labs(title = "Risk prediction vs. Kernel density, 2018 burglaries",
           y = "% of Test Set Burglaries (per model)",
           x = "Risk Category") +
  theme_bw() +
      theme(axis.text.x = element_text(angle = 45, vjust = 0.5))
```