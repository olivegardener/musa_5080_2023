---
title: "Predictive Tree Work"
author: "Oliver Atwood"
date: "2023-10-17"
output: html_document
---
Your job is to build a version of the spatial risk model seen in the book (and in the Week 6 "Predictive Policing" lab) for a different outcome that likely suffers from more selection bias than burglary. You can build this model in Chicago or any other city with sufficient open data resources.

Please also add at least two new independent features not used in Chapter 5, and iteratively build models until you have landed on one that optimizes for accuracy and generalizability.

Your final deliverable should be in R markdown form with code (please hide the code blocks). Please provide the following materials with brief annotations (please donâ€™t forget this):






The purpose of adapting this predictive policing algorithm to predict Street Tree 311 calls is to facilitate preventative care and avoid property damage from fallen trees. It must be said that this theoretical care is highly aspirational, as there are currently thousands of tree work calls waiting to be resolved. However, if the city were able to clear the backlog of tree maintenance requests, this tool could theoretically be used to focus preventative tree care efforts in areas most likely to experience 311 calls about street trees.




## Setup
Clear Environment
```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())

source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

```

Load Libraries
```{r}
library(tidyverse)
library(sf)
library(RSocrata)
library(viridis)
library(spatstat)
library(raster)
library(spdep)
library(FNN)
library(grid)
library(gridExtra)
library(knitr)
library(kableExtra)
library(tidycensus)
library(classInt)   # for KDE and ML risk class intervals
library(tigris)
library(arcpullr)
library(units)
library(zip)

```

Set Parameters
```{r}
# Coordinate System
coordinate_system <- 2272

# Resolution
resolution <- 1000

#Load Boundary File
philly_boundary <- counties(state = 42) %>% 
  filter(NAME == "Philadelphia") %>% 
  st_transform(crs = coordinate_system)

```
## Data Wrangling
Filter 311 requests data
```{r}
# # 311 Requests API (needs work)
# # StreetTreeRequests_ALL <- get_spatial_layer("https://services.arcgis.com/fLeGjb7u4uXqeF9q/ArcGIS/rest/services/philly311__public_cases/FeatureServer/0/") %>%
# #   filter(service_name == 'Street Trees') %>%
# #   filter(lat != "") %>%
# #   st_as_sf(coords = c("lon", "lat"), crs = 4326) %>%
# #   st_transform(crs = coordinate_system)
#   
# # 311 Requests (by year)
# # StreetTreeRequests <- st_read("/Users/oliveratwood/Downloads/Philly 311/public_cases_fc2022.csv") %>%
# # StreetTreeRequests <- st_read("/Users/oliveratwood/Downloads/Philly 311/public_cases_fc2021.csv") %>%
# StreetTreeRequests <- st_read("/Users/oliveratwood/Downloads/Philly 311/public_cases_fc2019.csv") %>%
# # StreetTreeRequests <- st_read("/Users/oliveratwood/Downloads/Philly 311/public_cases_fc2018.csv") %>%
#   filter(service_name == 'Street Trees') %>%
#   filter(lat != "") %>%
#   st_as_sf(coords = c("lon", "lat"), crs = 4326) %>%
#   st_transform(crs = coordinate_system)
# 
# # Specify the file path where you want to save the GeoJSON file
# # file_path <- "/Users/oliveratwood/Documents/GitHub/musa_5080_2023/Week_6/PredictiveTreeing/Data/StreetTreeRequests2022.geojson"
# # file_path <- "/Users/oliveratwood/Documents/GitHub/musa_5080_2023/Week_6/PredictiveTreeing/Data/StreetTreeRequests2021.geojson"
# file_path <- "/Users/oliveratwood/Documents/GitHub/musa_5080_2023/Week_6/PredictiveTreeing/Data/StreetTreeRequests2019.geojson"
# # file_path <- "/Users/oliveratwood/Documents/GitHub/musa_5080_2023/Week_6/PredictiveTreeing/Data/StreetTreeRequests2018.geojson"
# 
# # Write the data frame as GeoJSON
# st_write(StreetTreeRequests, file_path)
# 
# # LOAD +WRITE TREE INVENTORY
# trees <- get_spatial_layer("https://services.arcgis.com/fLeGjb7u4uXqeF9q/arcgis/rest/services/PPR_Tree_Inventory_2022/FeatureServer/0/") %>%
#   filter(TREE_DBH > 0, TREE_DBH < 100) %>% 
#   st_transform(crs = coordinate_system) %>% 
#   st_join(philly_boundary) %>%
#   mutate(Legend = "PhillyTrees") %>%
#   mutate(Legend2 = "DBH")
# 
# # Write the data frame as GeoJSON
# file_path <- "/Users/oliveratwood/Documents/GitHub/musa_5080_2023/Week_6/PredictiveTreeing/Data/PPR_Tree_Inventory_OA.geojson"
# st_write(trees, file_path)
# 
# # Compress the file using gzip
# system(paste("gzip", file_path))
```

Load data for independent variables
```{r}
# Load and Unzip tree inventory
file_path_gz <- "https://raw.githubusercontent.com/olivegardener/musa_5080_2023/main/Week_6/PredictiveTreeing/Data/PPR_Tree_Inventory_OA.geojson.gz"

# Unzipping and reading the content
con <- gzcon(file(file_path_gz, "rb"))
geojson_content <- readLines(con)
close(con)

# Write uncompressed content to a temporary file
temp_file <- tempfile(fileext = ".geojson")
writeLines(geojson_content, temp_file)

# Read the GeoJSON from the temporary file
trees <- st_read(temp_file, quiet = TRUE)

# Delete the temporary file after reading it
unlink(temp_file)


#2018 ACS data for Philly
tracts2018 <- 
  get_acs(geography = "tract", 
          variables = c("B25026_001E",
                        "B19013_001E",
                        "DP04_0046PE"), 
          year=2018, state=42, county="101", 
          geometry=TRUE, output="wide") %>%
  st_transform(coordinate_system) %>%
  rename(TotalPop = B25026_001E, 
         MedHHInc = B19013_001E, 
         HomeOwnRate = DP04_0046PE) %>% 
  mutate(area = st_area(geometry)) %>%
  mutate(year = "2018", 
         PopDensity = drop_units(TotalPop / (area / 2.788e+7))) %>% 
  mutate(HomeOwnRate = ifelse(is.na(HomeOwnRate), 0, HomeOwnRate)) %>% 
  mutate(MedHHInc = ifelse(is.na(MedHHInc), 0, MedHHInc)) %>% 
  dplyr::select(-NAME, -TotalPop, -area, -starts_with("D"), -starts_with("B"))

neighborhoods <- get_spatial_layer("https://services1.arcgis.com/a6oRSxEw6eIY5Zfb/ArcGIS/rest/services/Philadelphia_Neighborhoods/FeatureServer/0")%>%
  st_transform(crs = 2272)

# community_landcare <- get_spatial_layer("https://services.arcgis.com/fLeGjb7u4uXqeF9q/arcgis/rest/services/PHS_CommunityLandcare/FeatureServer/0/") %>% 
#   st_transform(crs = 2272)
# landcare_maintenance <- get_spatial_layer("https://services.arcgis.com/fLeGjb7u4uXqeF9q/arcgis/rest/services/PHS_PhilaLandCare_Maintenance/FeatureServer/0/") %>%
#   st_transform(crs = 2272)
# 
# community_landcare <- st_centroid(community_landcare)
# community_landcare <- st_geometry(community_landcare)
# community_landcare <- st_sf(geometry = community_landcare)
# 
# landcare_maintenance <- st_centroid(landcare_maintenance)
# landcare_maintenance <- st_geometry(landcare_maintenance)
# landcare_maintenance <- st_sf(geometry = landcare_maintenance)
# 
# landcare_lots <- rbind(community_landcare, landcare_maintenance)


gdb_path <- "/Users/oliveratwood/Documents/GitHub/musa_5080_2023/Midterm/data/Features4PPA.gdb"
layers <- ogrListLayers(gdb_path)

Vacant_Lots <- sf::st_read(gdb_path, layer = "VacantLots")
```

Load in Selected 311 Data
```{r}
# Load Street Tree 311 Requests (previously cleaned, see commented code above)
StreetTreeRequests <- st_read("https://raw.githubusercontent.com/olivegardener/musa_5080_2023/main/Week_6/PredictiveTreeing/Data/StreetTreeRequests2018.geojson")

```

A map of your outcome of interest in point form, with some description of what, when, and why you think selection bias may be an issue.

In this analysis, I have adapted a methodology originally used to predict the occurrence of crime to predict the likelihood of 311 calls for street trees. It is likely that selection bias plays a similar role in this dynamic as it does in predictive policing. 311 calls for street trees are likely to occur are, obviously, in areas with more trees. It is possible that certain individuals are more likely to place calls than others neighborhoods. It is also likely that areas with higher population density are more likely to have individuals making 311 calls about street trees.

```{r fig.width=6, fig.height=4}
# uses grid.arrange to organize independent plots
grid.arrange(ncol=2,
ggplot() + 
  geom_sf(data = philly_boundary) +
  geom_sf(data = StreetTreeRequests, colour="orange", size=0.1, show.legend = "point") +
  labs(title= "Tree 311 Calls, Philadelphia - 2022") +
  mapTheme(title_size = 10),

ggplot() + 
  geom_sf(data = philly_boundary, fill = "grey40") +
  stat_density2d(data = data.frame(st_coordinates(StreetTreeRequests)), 
                 aes(X, Y, fill = ..level.., alpha = ..level..),
                 size = 0.01, bins = 40, geom = 'polygon') +
  scale_fill_viridis() +
  scale_alpha(range = c(0.00, 0.35), guide = FALSE) +
  labs(title = "Density of Tree 311 Calls") +
  mapTheme(title_size = 10) + theme(legend.position = "none"))
```

```{r}
## using {sf} to create the grid
## Note the `.[philly_boundary] %>% ` line. This is needed to clip the grid to our data
fishnet <- 
  st_make_grid(philly_boundary,
               cellsize = resolution, 
               square = TRUE) %>%
               # hexagon = TRUE) %>% #option for a hexagonal grid
  .[philly_boundary] %>%            # fast way to select intersecting polygons
  st_sf() %>%
  mutate(uniqueID = 1:n())

```
A map of your outcome joined to the fishnet.

```{r, fig.width=10}
## add a value of 1 to each tree request, sum them with aggregate
tree_net <- 
  dplyr::select(StreetTreeRequests) %>% 
  mutate(countRequests = 1) %>% 
  aggregate(., fishnet, sum) %>%
  mutate(countRequests = replace_na(countRequests, 0),
         uniqueID = 1:n(),
         cvID = sample(round(nrow(fishnet) / 24), 
                       size=nrow(fishnet), replace = TRUE))

ggplot() +
  geom_sf(data = tree_net, aes(fill = countRequests), color = NA) +
  scale_fill_viridis() +
  labs(title = "Count of Street Tree Service Requests for the Fishnet") +
  mapTheme()
```

## Modeling Spatial Features

```{r}
vars_net <- trees %>%
  st_join(fishnet, join=st_within) %>%
  st_drop_geometry() %>%
  group_by(uniqueID, Legend) %>%
  summarize(count = n(), 
            sum_TREE_DBH = sum(TREE_DBH, na.rm = TRUE)) %>% # Calculate sum of TREE_DBH
  left_join(fishnet, ., by = "uniqueID") %>%
  spread(Legend, count, fill=0) %>% 
  dplyr::select(-`<NA>`) %>%
  ungroup() %>% 
  mutate(sum_TREE_DBH = ifelse(is.na(sum_TREE_DBH), 0, sum_TREE_DBH))  # Replace NA values with 0

```

## Nearest Neighbor Feature

```{r}
# convinience to reduce length of function names.
st_c    <- st_coordinates
st_coid <- st_centroid

## create NN from abandoned cars
vars_net <- vars_net %>%
    mutate(trees.nn = nn_function(st_c(st_coid(vars_net)), 
                                           st_c(trees),
                                           k = 5))
```
## Join NN feature to our fishnet

Since the counts were aggregated to each cell by `uniqueID` we can use that to join the counts to the fishnet.

```{r}
## important to drop the geometry from joining features
final_net <-
  left_join(tree_net, st_drop_geometry(vars_net), by="uniqueID") 

```

### Join in areal data

Using spatial joins to join *centroids* of fishnets to polygon for neighborhoods and districts.

> What issues arise when we try to join polygons to polygons in space?

```{r}

final_net <- st_centroid(final_net) %>%
    st_join(dplyr::select(tracts2018, MedHHInc), by = "uniqueID") %>%
    st_join(dplyr::select(tracts2018, HomeOwnRate), by = "uniqueID") %>%
    st_join(dplyr::select(tracts2018, PopDensity), by = "uniqueID") %>%
    st_join(dplyr::select(neighborhoods, NAME), by = "uniqueID") %>%
      st_drop_geometry() %>%
      left_join(dplyr::select(final_net, geometry, uniqueID)) %>%
      st_sf() %>%
  na.omit()

final_net <- final_net %>% 
  rename(neighborhoods = NAME)

# for live demo
# mapview::mapview(final_net, zcol = "District")
```

A small multiple map of your risk factors in the fishnet (counts, distance and/or other feature engineering approaches).

```{r, fig.width=10}
final_net.long <- final_net %>% 
  dplyr::select(-neighborhoods, -cvID)

final_net.long <- gather(final_net.long, Variable, value, -geometry, -uniqueID, -countRequests)
vars <- unique(final_net.long$Variable)

vars <- unique(final_net.long$Variable)
mapList <- list()

for(i in seq_along(vars)){
  mapList[[i]] <- 
    ggplot() +
      geom_sf(data = filter(final_net.long, Variable == vars[i]), aes(fill=value), colour=NA) +
      scale_fill_viridis(name="") +
      labs(title=vars[i]) +
      mapTheme()
}


do.call(grid.arrange,c(mapList, ncol=3, top="Risk Factors by Fishnet"))
```


## Local Moran's I for fishnet grid cells

using {spdep} package to to build neighborhood weights and list to calculate local Moran's I.

Note that the code here is *different* than in the book - it has been updated to keep up with changes in packages.

> What is the difference between local and global Moran's I?

A little in depth version of the chunk below can be found:

Mendez C. (2020). Spatial autocorrelation analysis in R. R Studio/RPubs. Available at <https://rpubs.com/quarcs-lab/spatial-autocorrelation>

```{r}
## generates warnings from PROJ issues
## {spdep} to make polygon to neighborhoods... 
final_net.nb <- poly2nb(as_Spatial(final_net), queen=TRUE)
## ... and neighborhoods to list of weigths
final_net.weights <- nb2listw(final_net.nb, style="W", zero.policy=TRUE)

# print(final_net.weights, zero.policy=TRUE)
```

```{r}
## see ?localmoran
local_morans <- localmoran(final_net$countRequests, final_net.weights, zero.policy=TRUE) %>% 
  as.data.frame()

# join local Moran's I results to fishnet
final_net.localMorans <- 
  cbind(local_morans, as.data.frame(final_net)) %>% 
  st_sf() %>%
  dplyr::select(TreeCount = PhillyTrees, 
                Local_Morans_I = Ii, 
                P_Value = `Pr(z != E(Ii))`) %>%
  mutate(Significant_Hotspots = ifelse(P_Value <= 0.001, 1, 0)) %>%
  gather(Variable, Value, -geometry)
  
```
### Plotting local Moran's I results

This is a complex code chunk - it's a loop which builds ggplots of local Moran's for each of your `vars`

Local Moranâ€™s I-related small multiple map of your outcome (see 5.4.1)
A small multiple scatterplot with correlations.
A histogram of your dependent variable.
A small multiple map of model errors by random k-fold and spatial cross validation.
A table of MAE and standard deviation MAE by regression.
A table of raw errors by race context for a random k-fold vs. spatial cross validation regression.
The map comparing kernel density to risk predictions for the next yearâ€™s crime.
The bar plot making this comparison.
Two paragraphs on why or why not you would recommend your algorithm be put into production.

```{r, fig.width=10}
## This is just for plotting
vars <- unique(final_net.localMorans$Variable)
varList <- list()

for(i in vars){
  varList[[i]] <- 
    ggplot() +
      geom_sf(data = filter(final_net.localMorans, Variable == i), 
              aes(fill = Value), colour=NA) +
      scale_fill_viridis(name="") +
      labs(title=i) +
      mapTheme(title_size = 14) + theme(legend.position="bottom")}

do.call(grid.arrange,c(varList, ncol = 4, top = "Local Morans I statistics, Street Tree 311 Requests"))
```

## Distance to Hot spot

Using NN distance to a hot spot location

```{r}
# generates warning from NN
final_net <- final_net %>% 
  mutate(Trees.isSig = 
           ifelse(local_morans[,5] <= 0.001, 1, 0)) %>%
  mutate(Trees.isSig.dist = 
           nn_function(st_c(st_coid(final_net)),
                       st_c(st_coid(filter(final_net, 
                                           Trees.isSig == 1))), 
                       k = 1))

```



### Plot NN distance to hot spot

```{r, fig.width=10}
ggplot() +
      geom_sf(data = final_net, aes(fill=Trees.isSig.dist), color=NA) +
      scale_fill_viridis(name="NN Distance") +
      labs(title="Street Tree 311 Requests NN Distance") +
      mapTheme()
```

## Modeling and CV

Leave One Group Out CV on spatial features

```{r results='hide'}

## define the variables we want
reg.ss.vars <- c("trees.nn", "Trees.isSig.dist")

# reg.ss.vars <- c("sum_TREE_DBH", "MedHHInc", "HomeOwnRate", "PopDensity", "Trees.isSig.dist")


## RUN REGRESSIONS
reg.ss.spatialCV <- crossValidate(
  dataset = final_net,
  id = "uniqueID",                           
  dependentVariable = "countRequests",
  indVariables = reg.ss.vars) %>%
    dplyr::select(cvID = uniqueID, countRequests, Prediction, geometry)
```


```{r}
# calculate errors by NEIGHBORHOOD
error_by_reg_and_fold <- 
  reg.ss.spatialCV %>%
    group_by(cvID) %>% 
    summarize(Mean_Error = mean(Prediction - countRequests, na.rm = T),
              MAE = mean(abs(Mean_Error), na.rm = T),
              SD_MAE = mean(abs(Mean_Error), na.rm = T)) %>%
  ungroup()

error_by_reg_and_fold %>% 
  arrange(desc(MAE))
error_by_reg_and_fold %>% 
  arrange(MAE)

## plot histogram of OOF (out of fold) errors
error_by_reg_and_fold %>%
  ggplot(aes(MAE)) + 
    geom_histogram(bins = 30, colour="black", fill = "#FDE725FF") +
  scale_x_continuous(breaks = seq(0, 11, by = 1)) + 
    labs(title="Distribution of MAE", subtitle = "LOGO-CV",
         x="Mean Absolute Error", y="Count") 
```

## Density vs predictions

The `spatstat` function gets us kernal density estimates with varying search radii.

Note that the code here is *different* than in the book - it has been updated to keep up with changes in packages.

```{r, fig.width=10}
# demo of kernel width
TreeCall_ppp <- as.ppp(st_coordinates(StreetTreeRequests), W = st_bbox(final_net))
TreeCall.1000 <- density.ppp(TreeCall_ppp, 1000)
TreeCall.1500 <- density.ppp(TreeCall_ppp, 1500)
TreeCall.2000 <- density.ppp(TreeCall_ppp, 2000)
TreeCall_KD.df <- rbind(
  mutate(data.frame(rasterToPoints(mask(raster(TreeCall.1000), as(neighborhoods, 'Spatial')))), Legend = "1000 Ft."),
  mutate(data.frame(rasterToPoints(mask(raster(TreeCall.1500), as(neighborhoods, 'Spatial')))), Legend = "1500 Ft."),
  mutate(data.frame(rasterToPoints(mask(raster(TreeCall.2000), as(neighborhoods, 'Spatial')))), Legend = "2000 Ft.")) 

TreeCall_KD.df$Legend <- factor(TreeCall_KD.df$Legend, levels = c("1000 Ft.", "1500 Ft.", "2000 Ft."))

ggplot(data=TreeCall_KD.df, aes(x=x, y=y)) +
  geom_raster(aes(fill=layer)) + 
  facet_wrap(~Legend) +
  coord_sf(crs=st_crs(final_net)) + 
  scale_fill_viridis(name="Density") +
  labs(title = "Kernel density with 3 different search radii") +
  mapTheme(title_size = 14)
```

```{r, fig.width=10}

as.data.frame(TreeCall.1000) %>%
  st_as_sf(coords = c("x", "y"), crs = st_crs(final_net)) %>%
  aggregate(., final_net, mean) %>%
   ggplot() +
     geom_sf(aes(fill=value), color=NA) +
     geom_sf(data = sample_n(StreetTreeRequests, 1500), color = 'orange',size = .01) +
     scale_fill_viridis(name = "Density") +
     labs(title = "Kernel density of 2022 Street Tree 311 Requests") +
     mapTheme(title_size = 14)
```

## Get 2018 crime data

Let's see how our model performed relative to KD on the following year's data.

```{r}

StreetTreeRequests19 <- st_read("https://raw.githubusercontent.com/olivegardener/musa_5080_2023/main/Week_6/PredictiveTreeing/Data/StreetTreeRequests2019.geojson") %>%
  .[fishnet,]
```

```{r}

tree_KDE_sum <- as.data.frame(TreeCall.1000) %>%
  st_as_sf(coords = c("x", "y"), crs = st_crs(final_net)) %>%
  aggregate(., final_net, mean) 
kde_breaks <- classIntervals(tree_KDE_sum$value, 
                             n = 5, "fisher")
tree_KDE_sf <- tree_KDE_sum %>%
  mutate(label = "Kernel Density",
         Risk_Category = classInt::findCols(kde_breaks),
         Risk_Category = case_when(
           Risk_Category == 5 ~ "5th",
           Risk_Category == 4 ~ "4th",
           Risk_Category == 3 ~ "3rd",
           Risk_Category == 2 ~ "2nd",
           Risk_Category == 1 ~ "1st")) %>%
  cbind(
    aggregate(
      dplyr::select(StreetTreeRequests19) %>% mutate(TreeCallCount = 1), ., sum) %>%
    mutate(TreeCallCount = replace_na(TreeCallCount, 0))) %>%
  dplyr::select(label, Risk_Category, TreeCallCount)
```

Note that this is different from the book, where we pull a model out of a list of models we've created. For your homework, you'll be creating multiple models.

```{r}
ml_breaks <- classIntervals(reg.ss.spatialCV$Prediction, 
                             n = 5, "fisher")
tree_risk_sf <-
  reg.ss.spatialCV %>%
  mutate(label = "Risk Predictions",
         Risk_Category =classInt::findCols(ml_breaks),
         Risk_Category = case_when(
           Risk_Category == 5 ~ "5th",
           Risk_Category == 4 ~ "4th",
           Risk_Category == 3 ~ "3rd",
           Risk_Category == 2 ~ "2nd",
           Risk_Category == 1 ~ "1st")) %>%
  cbind(
    aggregate(
      dplyr::select(StreetTreeRequests19) %>% mutate(TreeCallCount = 1), ., sum) %>%
      mutate(TreeCallCount = replace_na(TreeCallCount, 0))) %>%
  dplyr::select(label,Risk_Category, TreeCallCount)
```

We don't do quite as well because we don't have very many features, but still pretty good.

```{r}
rbind(tree_KDE_sf, tree_risk_sf) %>%
  na.omit() %>%
  gather(Variable, Value, -label, -Risk_Category, -geometry) %>%
  ggplot() +
    geom_sf(aes(fill = Risk_Category), colour = NA) +
    geom_sf(data = sample_n(StreetTreeRequests19, 3000), size = .1, colour = "black", alpha = .1) +
    facet_wrap(~label, ) +
    scale_fill_viridis(discrete = TRUE) +
    labs(title="Comparison of Kernel Density and Risk Predictions",
         subtitle="2019 Street Tree 311 Call Predictions; 2018 Calls") +
    mapTheme(title_size = 14)
```

```{r}
rbind(tree_KDE_sf, tree_risk_sf) %>%
  st_drop_geometry() %>%
  na.omit() %>%
  gather(Variable, Value, -label, -Risk_Category) %>%
  group_by(label, Risk_Category) %>%
  summarize(countTreeCalls = sum(Value)) %>%
  ungroup() %>%
  group_by(label) %>%
  mutate(Pcnt_of_test_set_calls = countTreeCalls / sum(countTreeCalls)) %>%
    ggplot(aes(Risk_Category,Pcnt_of_test_set_calls)) +
      geom_bar(aes(fill=label), position="dodge", stat="identity") +
      scale_fill_viridis(discrete = TRUE, name = "Model") +
      labs(title = "Call prediction vs. Kernel density, 2019 Street Tree 311 Calls",
           y = "% of Test Set Calls (per model)",
           x = "Risk Category") +
  theme_bw() +
      theme(axis.text.x = element_text(angle = 45, vjust = 0.5))
```