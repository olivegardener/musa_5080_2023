---
title: "Targeting Housing Subsidy"
author: "Oliver Atwood"
date: "2023-11-20"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include=FALSE}
rm(list=ls())

knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(kableExtra)
library(caret)
library(knitr) 
library(pscl)
library(plotROC)
library(pROC)
library(lubridate)
library(gridExtra)
library(cowplot)

source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

options(scipen = 999)

```

# Introduction

### One paragraph on the motivation for the analysis.

Connecting people in need of public services to those services is an effective way to address a range of urban issues. In the post-Reagan era, government agencies must work within shoestring budgets. It is therefore paramount that these agencies effectively target their outreach to maximize impact and minimize cost. 
<br>
For the Department of Housing and Community Development to effectively target its outreach, a new approach is needed. Current methods for outreach are inefficient and ad-hoc, whereas a model-based approach can better target users that are more likely to enroll in programs and take housing repair subsidy credits. Through the development of models based on historical data, this analysis finds patterns of which individuals in the city are most likely to complete the full application process and take the credit, such that outreach can be optimized to maximize impact and minimize cost for the Department.


# Data

```{r}
palette5 <- c("#981FAC","#CB0F8B","#FF006A","#FE4C35","#FE9900")
palette4 <- c("#981FAC","#FF006A","#FE4C35","#FE9900")
palette3 <- c("#981FAC","#FF006A","#FE4C35")
palette2 <- c("#981FAC","#FF006A")

HousingSubsidy <- read.csv("https://raw.githubusercontent.com/olivegardener/musa_5080_2023/main/Week_7/TargetingHousingSubsidy/data/housingSubsidy.csv")

# Filter the dataframe
HousingSubsidy <- subset(HousingSubsidy, taxLien != 'yes')

```

```{r}
HousingSubsidy <- HousingSubsidy %>% 
  mutate(EnterProgram = ifelse(y == 'yes', 1, 0)) %>% 
  dplyr::select(-y_numeric)
```

Using previous records of outreach and outcomes, paired with internal (eg. age) and external factors (eg. inflation rate) related to individuals in question, we seek to build a model that effectively targets eligible homeowners by finding which factors make them more likely to enter the program and take the credit.

## Exploring data

```{r exploratory_continuous}
HousingSubsidy %>%
  dplyr::select(y, age, campaign, pdays, previous, unemploy_rate, cons.price.idx, cons.conf.idx, inflation_rate, spent_on_repairs) %>%
  gather(Variable, value, -y) %>%
    ggplot(aes(y, value, fill=y)) + 
      geom_bar(position = "dodge", stat = "summary", fun = "mean") + 
      facet_wrap(~Variable, scales = "free") +
      scale_fill_manual(values = palette2) +
      labs(x="Outcome", y="Value", 
           title = "Feature associations for Outcome of Entering Subsidy Program",
           subtitle = "(Continous Features)") +
      theme(legend.position = "none")
```
<br>
From these bar plots, it appears that there are notable differences between No (did not enter program) and Yes (did enter program) the following factors: campaign, inflation_rate, pdays, previous, and unemployment_rate. However, me must keep in mind that these are bar plots of the mean for each of these continuous variables, meaning that there may also be distinctions across the range of values for each variable that are not captured here. Let's make some line plots to reveal the continuous density for each of these variables.


```{r exploratory_continuous_density, message = FALSE, warning = FALSE}
HousingSubsidy %>%
    dplyr::select(y, age, campaign, pdays, previous, unemploy_rate, cons.price.idx, cons.conf.idx, inflation_rate, spent_on_repairs) %>%
    gather(Variable, value, -y) %>%
    ggplot() + 
    geom_density(aes(value, color=y), fill = "transparent") + 
    facet_wrap(~Variable, scales = "free") +
    scale_fill_manual(values = palette2) +
    labs(title = "Feature distributions for Outcome of Entering Subsidy Program",
         subtitle = "(Continous Features)",
         x="Value", y="Density")
```
<br>
These line plots visualize continuous variables. This reveals which ones have greater difference in values across the range of each variable, showing the degree of distinction between whether a given individual took the credit or did not. Of these 9 plots, inflation_rate appears to have the strongest distinction between individuals who enrolled in the program and those who did not. This density plot indicates that higher inflation rates are associated with lower enrollment and lower inflation rates are associated with higher enrollment.
<br>
<br>
Now let's examine the relationships between enrollment in the housing subsidy program and our categorical features.

```{r exploratory_binary, message = FALSE, warning = FALSE, fig.width=10}
HousingSubsidy %>%
    dplyr::select(y, job, marital, education, taxLien, mortgage, taxbill_in_phl, contact, month, day_of_week, campaign, poutcome) %>%
    gather(Variable, value, -y) %>%
    count(Variable, value, y) %>%
      ggplot(., aes(value, n, fill = y)) +   
        geom_bar(position = "dodge", stat="identity") +
        facet_wrap(~Variable, scales="free") +
        scale_fill_manual(values = palette2) +
        labs(x="Outcome", y="Value",
             title = "Feature associations for Outcome of Entering Subsidy Program",
             subtitle = "Categorical Features") +
        theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
<br>
These plots show differences across categorical variables. For example, the plot of the "contact" variable shows that participants are be more likely to join the program if they are contacted by cell phone as opposed to by telephone. With these variables, it is important to also compare across the category in addition to looking at the difference between outcomes.

# Model

```{r}
set.seed(3456)
trainIndex <- createDataPartition(HousingSubsidy$y, p = .65,
                                  list = FALSE,
                                  times = 1)
SubsidyTrain <- HousingSubsidy[ trainIndex,]
SubsidyTest  <- HousingSubsidy[-trainIndex,]

```

```{r}
SubsidyModel <- glm(EnterProgram ~ .,
                  data=SubsidyTrain %>% 
                    dplyr::select(-y, -X),
                  family="binomial" (link="logit"))

```
The model type we will be using for our prediction is called a logistic regression model, which allows us to model binary outcomes (i.e., yes or no). We can determine how well we do at engineering new features by examining the the AIC score that is generated by the regression output - for this metric, lower is better. Our first model takes a 'kitchen sink' approach, using all of the features in our sample data. This model was not particularly effective at predicting people entering the program.
<br>
<br>
To improve model performance, we will engineer some additional features, based on existing variables in our data set and test how these improve model performance.


## Feature Engineering

```{r}
HousingSubsidy2 <- HousingSubsidy %>% 
  mutate(MarchOrNot = ifelse(month == "mar", 1, 0)) %>% 
  mutate(SingleOrNot = ifelse(marital == "single", 1, 0)) %>% 
  mutate(WedOrNot = ifelse(day_of_week == "wed", 1, 0)) %>% 
  dplyr::select(-month, -job, -education, -marital, -taxLien, -pdays, -taxbill_in_phl, -mortgage, -campaign, -previous, -day_of_week, -X)
```

```{r}
set.seed(3456)
trainIndex2 <- createDataPartition(HousingSubsidy2$y, p = .65,
                                  list = FALSE,
                                  times = 1)
SubsidyTrain2 <- HousingSubsidy2[ trainIndex2,]
SubsidyTest2  <- HousingSubsidy2[-trainIndex2,]

```

```{r}
SubsidyModel2 <- glm(EnterProgram ~ .,
                  data=SubsidyTrain2 %>% 
                    dplyr::select(-y),
                  family="binomial" (link="logit"))

```

```{r}
HousingSubsidy3 <- HousingSubsidy %>% 
  mutate(SingleOrNot = ifelse(marital == "single", 1, 0),
         season = 
           case_when(month == "dec" | month == "jan" | month == "feb"~ "Winter",
                     month == "mar" | month == "apr" | month == "may"~ "Spring",
                     month == "jun" | month == "jul" | month == "aug"~ "Summer",
                     month == "sep" | month == "oct" | month == "nov"~ "Fall"
           ),
         campaign_cat = 
           case_when(campaign == 1 ~ "one",
                     campaign == 2 ~ "two",
                     campaign == 3 ~ "three",
                     campaign >= 4 ~ "four+"),
         pdays_cat =
           case_when(pdays == 999 ~ "No contact",
                     pdays <= 14 ~ "Within 2 weeks",
                     pdays > 14 & pdays <=30 ~ "Between 2 weeks and 1 month"),
         inflation_split =
           case_when(inflation_rate >= 3 ~ "3%+",
                     inflation_rate < 3 ~ "<3%")
         ) %>% 
  dplyr::select(-month, -job, -education, -marital, -taxLien, -pdays, -taxbill_in_phl, -mortgage, -campaign, -previous, -day_of_week, -inflation_rate, -X)
```

```{r create_partition}
set.seed(3456)
trainIndex3 <- createDataPartition(HousingSubsidy3$y, p = .65,
                                  list = FALSE,
                                  times = 1)
SubsidyTrain3 <- HousingSubsidy3[ trainIndex3,]
SubsidyTest3  <- HousingSubsidy3[-trainIndex3,]

```

```{r run_model}
SubsidyModel3 <- glm(EnterProgram ~ .,
                  data=SubsidyTrain3 %>% 
                    dplyr::select(-y),
                  family="binomial" (link="logit"))

```

### Feature Engineering Interpretation
<br>
In engineering these features, we started by pulling out some of the variables considered statistically significant in the model and dropping most of the other variables. In this case, March, Wednesdays, and a marital status of "Single" appear to be statistically significant, so we created binary variables for each of these, 'MarchOrNot', 'WedOrNot', and 'SingleOrNot', respectively. The original columns for each of these factors were then omitted.
<br>
<br>
Given that the previously described features that we created, mainly Wednesdays and March, could be a reflection of quirks in the data, and that this approach of over-pursuing significance may improve accuracy of the model, but is bad practice for generalizability we took a different approach for a third model, this time developing several additional features that may more accurately reflect phenomena in the data. In this case, we keep the marital status variable and add the following:
<br>
<br>
* Season, to capture more generalizable categories for time of year (winter, spring, summer, fall),
* Campaign category, to provide a count of the number of times that an individual was contacted during this sample campaign data as once, twice, three times, or four or more times,
* pdays category for the number of days that an individual wa s contacted from a previous program as never, within the previous two weeks, or between two to four weeks, and
* Inflation split, which classifies inflation as over or under 3%, which is roughly considered high or low inflation based on the U.S. Federal Reserve's long term inflation goals.

### Regression Summaries
```{r}
summary(SubsidyModel)

summary(SubsidyModel2)

summary(SubsidyModel3)

```
<br>
Here, we run all three models to compare AIC scores for each and see which model better reflects our data. The AIC score for the kitchen sink model is 1579, for the second model is 1551. and for the third model 1560. While the second model technically has the lowest score, it is possible that the significance of March or Wednesdays could be a reflection of quirks in the data gathering process and are difficult to decipher without additional context that is not reflected in the data set. Because of this possibility, we decided to move forward with the third model in our analysis, since this model has a lower (and therefore better) AIC value compared to the kitchen sink model while using more generalizable features.
<br>
# Cross Validation

### Cross validate both models; compare and interpret two faceted plots of ROC, Sensitivity and Specificity.


```{r cv1}
ctrl1 <- trainControl(method = "cv", number = 100, classProbs=TRUE, summaryFunction=twoClassSummary)

cvFit1 <- train(y ~ .,
                  data=HousingSubsidy %>% 
                    dplyr::select(-EnterProgram, -X), 
                method="glm", family="binomial",
                metric="ROC", trControl = ctrl1)

cvFit1
```
```{r goodness_metrics1, message = FALSE, warning = FALSE}
dplyr::select(cvFit1$resample, -Resample) %>%
  gather(metric, value) %>%
  left_join(gather(cvFit1$results[2:4], metric, mean)) %>%
  ggplot(aes(value)) + 
    geom_histogram(bins=35, fill = "#FF006A") +
    facet_wrap(~metric) +
    geom_vline(aes(xintercept = mean), colour = "#981FAC", linetype = 3, size = 1.5) +
    scale_x_continuous(limits = c(0, 1)) +
    labs(x="Goodness of Fit", y="Count", title="CV Goodness of Fit Metrics",
         subtitle = "Across-fold mean reprented as dotted lines")

```



```{r cv3}
ctrl3 <- trainControl(method = "cv", number = 100, classProbs=TRUE, summaryFunction=twoClassSummary)

cvFit3 <- train(y ~ .,
                  data=HousingSubsidy3 %>% 
                    dplyr::select(-EnterProgram), 
                method="glm", family="binomial",
                metric="ROC", trControl = ctrl3)

cvFit3
```

```{r goodness_metrics3, message = FALSE, warning = FALSE}
dplyr::select(cvFit3$resample, -Resample) %>%
  gather(metric, value) %>%
  left_join(gather(cvFit3$results[2:4], metric, mean)) %>%
  ggplot(aes(value)) + 
    geom_histogram(bins=35, fill = "#FF006A") +
    facet_wrap(~metric) +
    geom_vline(aes(xintercept = mean), colour = "#981FAC", linetype = 3, size = 1.5) +
    scale_x_continuous(limits = c(0, 1)) +
    labs(x="Goodness of Fit", y="Count", title="CV Goodness of Fit Metrics",
         subtitle = "Across-fold mean reprented as dotted lines")

```

We ran cross-validation tests on model 1 and model 3 to examine outcomes for the ROC curve, sensitivity, and specificity across 100 different iterations (folds). The ROC curve is an indicator of goodness of fit for the model and will be explored in the following section. Sensitivity, as defined earlier, is the true positive rate while specificity is the true negative rate. Sensitivity and specificity measure how accurately the model captures actual phenomena in the data. The closer each cross validation metric is to the mean, the more generalizable the model is. Both model 1 and model 3 have similar metrics, with  high sensitivity and relatively low specificity - however, our engineered regression model is slightly more sensitive, and so is better at predicting true positives in the data.

# ROC Curve

```{r testProbs}
testProbs <- data.frame(Outcome = as.factor(SubsidyTest$EnterProgram),
                        Probs = predict(SubsidyModel, SubsidyTest, type= "response"))

testProbs2 <- data.frame(Outcome = as.factor(SubsidyTest2$EnterProgram),
                        Probs = predict(SubsidyModel2, SubsidyTest2, type= "response"))

testProbs3 <- data.frame(Outcome = as.factor(SubsidyTest3$EnterProgram),
                        Probs = predict(SubsidyModel2, SubsidyTest2, type= "response"))

```

## Area Under Curve

```{r auc, message = FALSE, warning = FALSE}
pROC::auc(testProbs3$Outcome, testProbs3$Probs)

```

## ROC Curve Plot

```{r roc_curve, warning = FALSE, message = FALSE}
ggplot(testProbs3, aes(d = as.numeric(Outcome), m = Probs)) +
  geom_roc(n.cuts = 50, labels = FALSE, colour = "#FE9900") +
  style_roc(theme = theme_grey) +
  geom_abline(slope = 1, intercept = 0, size = 1.5, color = 'grey') +
  labs(title = "ROC Curve - Housing Subsidy Model")
```

The ROC curve is a reflection of our two most important metrics - true positive responses and false positive responses. False positives are people that we marketed to that did not attempt to enter the housing credit program.

This curve allows us to understand the different thresholds of correctly versus incorrectly predicting that someone will enter the program. Our goal is for the area under the curve of our line of predicted outcome thresholds to be near 1 but not at 1. In our case, model 3 achieved an Area Under the Curve value of 0.8183, representing a good amount of fitness. This ROC curve shows us that the model is better than random prediction, since the curve is above the diagonal line, which represents a coin flip. It also shows that the model is not over-fit, since the curve is a reasonable distance from the top left corner of the plot.

### Cost Benefit Analysis

### Confusion Matrix

```{r thresholds}
testProbs3 <- 
  testProbs3 %>%
  mutate(predOutcome  = as.factor(ifelse(testProbs3$Probs > 0.5 , 1, 0)))
```

```{r confusion_matrix}
caret::confusionMatrix(testProbs3$predOutcome, testProbs3$Outcome, 
                       positive = "1")

```

# Cost-Benefit Analysis

Finally, we developed a series of cost-benefits equations that reflect the four possibilities in our data, to determine outcomes and costs:
<br>
- True negative revenue “Predicted correctly homeowner would not take the credit, no marketing resources were allocated, and no credit was allocated.”: -\$0
- True positive revenue “Predicted correctly homeowner would enter credit program; allocated the marketing resources, and 25% ultimately achieved the credit”: -\$2,850 + \$5,000 = \$2,150 return for 25% of cases that take the credit. -\$2,850 for 75% of cases who did not take the credit.
- False negative revenue “Predicted that a homeowner would not take the credit but they did. These are likely homeowners who signed up for reasons unrelated to the marketing campaign”: \$0
- False positive revenue “Predicted incorrectly homeowner would take the credit; allocated marketing resources; no credit allocated.”: -\$2,850
<br>
The best outcomes are true positive and true negative outcomes - these scenarios are where our model was accurate. False negatives are a neutral outcome since they occur outside of our marketing campaign predictions. False positives are the worst outcome - we allocated marketing resources only to be incorrect in our prediction that the homeowner would enter the credit program.
<br>
For the purposes of cost, we define negative value investments as the marketing and administrative costs associated with outreach to individuals. We define positive value investments as the awarding of a credit to an individual, minus the marketing and administrative cost associated with it. By virtue of this program being a subsidy program, it does not directly generate revenue. Therefore, our goal is maximizing positive investments by the HCD while minimizing negative investments.


```{r cost_benefit}
cost_benefit_table <-
   testProbs3 %>%
      count(predOutcome, Outcome) %>%
      summarize(True_Negative = sum(n[predOutcome==0 & Outcome==0]),
                True_Positive = sum(n[predOutcome==1 & Outcome==1]),
                False_Negative = sum(n[predOutcome==0 & Outcome==1]),
                False_Positive = sum(n[predOutcome==1 & Outcome==0])) %>%
       gather(Variable, Count) %>%
       mutate(Revenue =
               ifelse(Variable == "True_Negative", Count * 0,
               ifelse(Variable == "True_Positive", (Count * .25 * (-2850+5000)) + (Count * .75 * -2850),
               ifelse(Variable == "False_Negative", Count * 0,
               ifelse(Variable == "False_Positive", (Count * -2850), 0))))) %>%
    bind_cols(data.frame(Description = c(
              "We correctly predicted no enrollment",
              "We correctly predicted enrollment",
              "We predicted no enrollment and person enrolled",
              "We predicted enrollment and customer did not enroll")))

kable(cost_benefit_table,
       caption = "Cost/Benefit Table") %>% kable_styling()
```

### Confusion Metric Outcomes by Threshold

```{r threshold, warning=FALSE}

whichThreshold <- 
  iterateThresholds(
     data=testProbs3, observedClass = Outcome, predictedProbs = Probs)

whichThreshold[1:5,]

whichThreshold <- 
  whichThreshold %>%
    dplyr::select(starts_with("Count"), Threshold) %>%
    gather(Variable, Count, -Threshold) %>%
    mutate(Revenue =
             case_when(Variable == "Count_TN"  ~ Count * 0,
                       Variable == "Count_TP"  ~ (Count * .25 * (-2850+5000)) + (Count * .75 * -2850) +
                                                 (-32 * (Count * .50)),
                       Variable == "Count_FN"  ~ Count * 0,
                       Variable == "Count_FP"  ~ (Count * -2850)))

whichThreshold %>%
  ggplot(.,aes(Threshold, Revenue, colour = Variable)) +
  geom_point() +
  scale_colour_manual(values = palette5[c(5, 1:3)]) +    
  labs(title = "Cost by confusion matrix type and threshold",
       y = "Cost") +
  plotTheme() +
  guides(colour=guide_legend(title = "Confusion Matrix")) 

```
<br>
These costs by threshold show that false positives are the largest driver of steep costs to the program and can lead to steep negative investments at lower thresholds. While minimizing these costs is important for us in our role as the municipal government, this metric does not capture the number of credits awarded, only program costs.
<br>

### Plots of Revenue and Credits Thresholds

```{r threshold rev plot}

whichThreshold_revenue <- 
  whichThreshold %>% 
    mutate(Count = ifelse(Variable == "Count_TP", (Count * .25),0)) %>% 
    group_by(Threshold) %>% 
    summarize(Total_Revenue = sum(Revenue),
              Total_Count_of_Credits = sum(Count)) 

#whichThreshold_revenue

fiftyThreshold = whichThreshold_revenue[50,]
optimalThreshold = whichThreshold_revenue[23,]
tableThreshold = rbind(fiftyThreshold, optimalThreshold)



rev_threshold <- whichThreshold_revenue %>%
  dplyr::select(Threshold, Total_Revenue) %>%
  gather(Variable, Value, -Threshold) %>%
  ggplot(aes(Threshold, Value, colour = Variable)) +
    geom_point() +
    geom_vline(xintercept = pull(arrange(optimalThreshold, -Total_Revenue)[1,1])) +
    scale_colour_manual(values = palette2) +
    plotTheme() +
    theme(legend.position = "none") +
    labs(title = "Total Investment Cost",
         subtitle = "Vertical line denotes chosen optimal threshold")

count_threshold <- whichThreshold_revenue %>%
  dplyr::select(Threshold, Total_Count_of_Credits) %>%
  gather(Variable, Value, -Threshold) %>%
  ggplot(aes(Threshold, Value, colour = Variable)) +
    geom_point() +
    geom_vline(xintercept = pull(arrange(optimalThreshold, -Total_Count_of_Credits)[1,1])) +
    scale_colour_manual(values = palette2) +
    plotTheme() +
    theme(legend.position = "none") +
    labs(title = "Total Count of Credits",
         subtitle = "Vertical line denotes chosen optimal threshold")

plot_grid(rev_threshold, count_threshold, ncol = 1, align = "v")
```
<br>
These plots align to show the cost and number of credits awarded at each threshold. Costs decrease as less credits are awarded, especially between the 0.0 and 0.15 thresholds, which is a reflection of false positive predictions decreasing. 

We also show what we determine to be the optimal threshold of 0.23, which we discuss in greater detail below. 

### Table of Revenue and Credits 50% and Optimal Thresholds

```{r table}

tableThreshold %>% kbl() %>% kable_minimal()

```

We have chosen our optimal threshold to be 0.23. While the marginal cost per credit increases - the cost is four times greater for less than three times the credits compared to the 0.50 threshold - we believe that this threshold optimizes the balancing of costs and benefits by limiting the steepest costs associated with a high number of false positive responses while effectively increasing the number of credits awarded.

# Conclusion

Conclude whether and why this model should or shouldn’t be put into production. What could make the model better? What would you do to ensure that the marketing materials resulted in a better response rate?

Our model excels in reducing false positive predictions, which are the worst outcome for our marketing efforts. Funding is more effective when it is spent on credits, worth \$5,000 for home repairs, rather than the $2,850 marketing and administrative expenses per homeowner.

Studies reveal that homeowners participating in the program and utilizing the credit typically benefit from a \$10,000 increase in property value. Additionally, neighboring homes collectively gain an average value of $56,000. These significant premiums highlight the value of our subsidies. However, we did not include these benefits in our cost-benefit calculation, as they are external factors dependent on the homeowner's transaction timing.

A clearly-defined budget from the HCD for this credit scheme would help us improve our model by allowing for cost-effective threshold setting. Furthermore, incorporating spatial data into the data set used to train the model could reveal geographic or demographic patterns potentially influencing program responses. If we found that certain areas were less likely to respond and join the program but ultimately not take the credit, we could adjust our marketing and administrative processes to better target outreach to those areas.

Finally, reducing administrative costs and the rate of participants not finalizing the credit is crucial. Algorithms such as this one can only go so far to improve the efficacy of government programs such as this one. Given the high expense per homeowner and issues like incomplete paperwork, enhancing the effectiveness of our counseling and information sessions is necessary. More focus on personalized reviews or support could resolve these problems. At the end of the day, increasing funding for the Department of Housing and Community Development will enable it to better serve the community.
