---
title: "Targeting Housing Subsidy"
author: "Oliver Atwood"
date: "2023-10-27"
output: html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include=FALSE}
rm(list=ls())

knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(kableExtra)
library(caret)
library(knitr) 
library(pscl)
library(plotROC)
library(pROC)
library(lubridate)
library(gridExtra)
library(cowplot)

source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

```

### One paragraph on the motivation for the analysis.

Connecting people in need of public services to those services is an effective way to address a range of urban issues. In the post-Reagan era, government agencies must work within shoestring budgets. It is therefore paramount that these agencies effectively target their outreach to maximize impact and minimize cost.


## Loading Data

```{r}
palette5 <- c("#981FAC","#CB0F8B","#FF006A","#FE4C35","#FE9900")
palette4 <- c("#981FAC","#FF006A","#FE4C35","#FE9900")
palette3 <- c("#981FAC","#FF006A","#FE4C35")
palette2 <- c("#981FAC","#FF006A")

HousingSubsidy <- read.csv("https://raw.githubusercontent.com/olivegardener/musa_5080_2023/main/Week_7/TargetingHousingSubsidy/data/housingSubsidy.csv")

# Filter the dataframe
HousingSubsidy <- subset(HousingSubsidy, taxLien != 'yes')

summary(HousingSubsidy)
```

```{r}
HousingSubsidy <- HousingSubsidy %>% 
  mutate(EnterProgram = ifelse(y == 'yes', 1, 0)) %>% 
  dplyr::select(-y_numeric)
```
### Develop and interpret data visualizations that describe feature importance/correlation.


## Exploring data
```{r exploratory_continuous}
HousingSubsidy %>%
  dplyr::select(y, age, campaign, pdays, previous, unemploy_rate, cons.price.idx, cons.conf.idx, inflation_rate, spent_on_repairs) %>%
  gather(Variable, value, -y) %>%
    ggplot(aes(y, value, fill=y)) + 
      geom_bar(position = "dodge", stat = "summary", fun = "mean") + 
      facet_wrap(~Variable, scales = "free") +
      scale_fill_manual(values = palette2) +
      labs(x="Click", y="Value", 
           title = "Feature associations with the likelihood of Entering the Housing Assistance Program",
           subtitle = "(continous outcomes)") +
      theme(legend.position = "none")
```
From these bar plots, it appears that there are notable differences between No and Yes for camaign, inflation_rate, pdays, previous, and unemployment_rate. However, me must keep in mind that these are bar plots of the mean for each of these continuous variables, meaning that there may also be distinctions across the range of values for each variable that are not captured here. Let's make some line plots to reveal the continuous density for each of these variables.

```{r exploratory_continuous_density, message = FALSE, warning = FALSE}
HousingSubsidy %>%
    dplyr::select(y, age, campaign, pdays, previous, unemploy_rate, cons.price.idx, cons.conf.idx, inflation_rate, spent_on_repairs) %>%
    gather(Variable, value, -y) %>%
    ggplot() + 
    geom_density(aes(value, color=y), fill = "transparent") + 
    facet_wrap(~Variable, scales = "free") +
    scale_fill_manual(values = palette2) +
    labs(title = "Feature distributions click vs. no click",
         subtitle = "(continous outcomes)")
```
Of these 9 plots, inflation_rate appears to have the strongest distinction between individuals who enrolled in the program and those who did not. This density plot indicates that higher inflation rates are associated with lower enrollment and lower inflation rates are associated with higher enrollment.
<br>
Now let's examine the relationships between enrollment in the housing subsidy program and our categorical features.
```{r exploratory_binary, message = FALSE, warning = FALS, fig.width=10}
HousingSubsidy %>%
    dplyr::select(y, job, marital, education, taxLien, mortgage, taxbill_in_phl, contact, month, day_of_week, campaign, poutcome) %>%
    gather(Variable, value, -y) %>%
    count(Variable, value, y) %>%
      ggplot(., aes(value, n, fill = y)) +   
        geom_bar(position = "dodge", stat="identity") +
        facet_wrap(~Variable, scales="free") +
        scale_fill_manual(values = palette2) +
        labs(x="Click", y="Value",
             title = "Feature associations with the likelihood of click",
             subtitle = "Categorical features") +
        theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## Create A Logistic Regression Model
###Split your data into a 65/35 training/test set.

```{r create_partition}
set.seed(3456)
trainIndex <- createDataPartition(HousingSubsidy$y, p = .65,
                                  list = FALSE,
                                  times = 1)
SubsidyTrain <- HousingSubsidy[ trainIndex,]
SubsidyTest  <- HousingSubsidy[-trainIndex,]

```

```{r run_model}
SubsidyModel <- glm(EnterProgram ~ .,
                  data=SubsidyTrain %>% 
                    dplyr::select(-y, -X),
                  family="binomial" (link="logit"))

summary(SubsidyModel)

```
The 'kitchen sink' model gives us an AIC of 736.02. let's try to improve this.

### Engineer new features that significantly increase the Sensitivity.

```{r}
HousingSubsidy2 <- HousingSubsidy %>% 
  mutate(MarchOrNot = ifelse(month == "mar", 1, 0)) %>% 
  mutate(SingleOrNot = ifelse(marital == "single", 1, 0)) %>% 
  mutate(WedOrNot = ifelse(day_of_week == "wed", 1, 0)) %>% 
  dplyr::select(-month, -job, -education, -marital, -taxLien, -pdays, -taxbill_in_phl, -mortgage, -campaign, -previous, -day_of_week, -X)
```

```{r create_partition}
set.seed(3456)
trainIndex2 <- createDataPartition(HousingSubsidy2$y, p = .65,
                                  list = FALSE,
                                  times = 1)
SubsidyTrain2 <- HousingSubsidy2[ trainIndex2,]
SubsidyTest2  <- HousingSubsidy2[-trainIndex2,]

```

```{r run_model}
SubsidyModel2 <- glm(EnterProgram ~ .,
                  data=SubsidyTrain2 %>% 
                    dplyr::select(-y),
                  family="binomial" (link="logit"))

```

```{r}
HousingSubsidy3 <- HousingSubsidy %>% 
  mutate(SingleOrNot = ifelse(marital == "single", 1, 0),
         season = 
           case_when(month == "dec" | month == "jan" | month == "feb"~ "Winter",
                     month == "mar" | month == "apr" | month == "may"~ "Spring",
                     month == "jun" | month == "jul" | month == "aug"~ "Summer",
                     month == "sep" | month == "oct" | month == "nov"~ "Fall"
           ),
         campaign_cat = 
           case_when(campaign == 1 ~ "one",
                     campaign == 2 ~ "two",
                     campaign == 3 ~ "three",
                     campaign >= 4 ~ "four+"),
         pdays_cat =
           case_when(pdays == 999 ~ "No contact",
                     pdays <= 14 ~ "Within 2 weeks",
                     pdays > 14 & pdays <=30 ~ "Between 2 weeks and 1 month"),
         inflation_split =
           case_when(inflation_rate >= 3 ~ "3%+",
                     inflation_rate < 3 ~ "<3%")
         ) %>% 
  dplyr::select(-month, -job, -education, -marital, -taxLien, -pdays, -taxbill_in_phl, -mortgage, -campaign, -previous, -day_of_week, -inflation_rate, -X)
```

```{r create_partition}
set.seed(3456)
trainIndex3 <- createDataPartition(HousingSubsidy3$y, p = .65,
                                  list = FALSE,
                                  times = 1)
SubsidyTrain3 <- HousingSubsidy3[ trainIndex3,]
SubsidyTest3  <- HousingSubsidy3[-trainIndex3,]

```

```{r run_model}
SubsidyModel3 <- glm(EnterProgram ~ .,
                  data=SubsidyTrain3 %>% 
                    dplyr::select(-y),
                  family="binomial" (link="logit"))

```

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
inflation rate above or below 3
p_days as cat

### Interpret your new features in one paragraph.
Given the outsized significance of the month of March, the day of the week of Wednesday, and single versus other marital statuses, I simplified these three categorical variables down to binaries: MarchOrNot, WedOrNot, and SingleOrNot, respectively. I then omitted the original columns they are derived from.


### Show a regression summary for both the kitchen sink and your engineered regression.
```{r}
summary(SubsidyModel)

summary(SubsidyModel2)

summary(SubsidyModel3)

```

### Cross validate both models; compare and interpret two facetted plots of ROC, Sensitivity and Specificity.
<br>
# Make Predictions
```{r testProbs}
testProbs <- data.frame(Outcome = as.factor(SubsidyTest$EnterProgram),
                        Probs = predict(SubsidyModel, SubsidyTest, type= "response"))

testProbs2 <- data.frame(Outcome = as.factor(SubsidyTest2$EnterProgram),
                        Probs = predict(SubsidyModel2, SubsidyTest2, type= "response"))

testProbs3 <- data.frame(Outcome = as.factor(SubsidyTest3$EnterProgram),
                        Probs = predict(SubsidyModel2, SubsidyTest2, type= "response"))

```

## Discussion 3

Look at the plot of our predicted probabilities for our observed clickers (`1`) and non-clickers (`0`). **Write a sentence or two about how you think our model is performing.**

```{r plot_testProbs, fig.width=10}
plot1 <- ggplot(testProbs, aes(x = Probs, fill = as.factor(Outcome))) + 
  geom_density(alpha = 0.7) +
  facet_grid(Outcome ~ .) +
  scale_fill_manual(values = palette2) +
  labs(x = "Click", y = "Density of probabilities",
       title = "Distribution of predicted probabilities\nby observed outcome") +
  theme(strip.text.x = element_text(size = 18),
        legend.position = "none")

plot2 <- ggplot(testProbs2, aes(x = Probs, fill = as.factor(Outcome))) + 
  geom_density(alpha = 0.7) +
  facet_grid(Outcome ~ .) +
  scale_fill_manual(values = palette2) +
  labs(x = "Click", y = "Density of probabilities",
       title = "Distribution of predicted probabilities\nby observed outcome") +
  theme(strip.text.x = element_text(size = 18),
        legend.position = "none")

grid.arrange(plot1, plot2, ncol=2)

```


### Output an ROC curve for your new model and interpret it.

# ROC Curve

The ROC curve, gives us another visual "goodness of fit" metric. One that is a bit more tricky. You want to have a curve that is "above" the y=x line, which is where your prediction rates for positives and negatives are "no better than a coin flip". If it's too "square" - you are probably over fit. The Area-Under-The-Curve or "AUC" calculation below will help guide your understanding of the ROC curve

```{r auc, message = FALSE, warning = FALSE}
pROC::auc(testProbs3$Outcome, testProbs3$Probs)

```

```{r roc_curve, warning = FALSE, message = FALSE}
ggplot(testProbs3, aes(d = as.numeric(Outcome), m = Probs)) +
  geom_roc(n.cuts = 50, labels = FALSE, colour = "#FE9900") +
  style_roc(theme = theme_grey) +
  geom_abline(slope = 1, intercept = 0, size = 1.5, color = 'grey') +
  labs(title = "ROC Curve - Housing Subsidy Model")
```
This ROC curve shows us that the model is better than random prediction, since the curve is above the diagonal line, which represents a coin flip. It is also not overfit, since it is 


# Cross Validation

```{r cv1}
ctrl1 <- trainControl(method = "cv", number = 100, classProbs=TRUE, summaryFunction=twoClassSummary)

cvFit1 <- train(y ~ .,
                  data=HousingSubsidy %>% 
                    dplyr::select(-EnterProgram, -X), 
                method="glm", family="binomial",
                metric="ROC", trControl = ctrl1)

cvFit1
```
```{r goodness_metrics1, message = FALSE, warning = FALSE}
dplyr::select(cvFit1$resample, -Resample) %>%
  gather(metric, value) %>%
  left_join(gather(cvFit1$results[2:4], metric, mean)) %>%
  ggplot(aes(value)) + 
    geom_histogram(bins=35, fill = "#FF006A") +
    facet_wrap(~metric) +
    geom_vline(aes(xintercept = mean), colour = "#981FAC", linetype = 3, size = 1.5) +
    scale_x_continuous(limits = c(0, 1)) +
    labs(x="Goodness of Fit", y="Count", title="CV Goodness of Fit Metrics",
         subtitle = "Across-fold mean reprented as dotted lines")

```

```{r cv2}
ctrl2 <- trainControl(method = "cv", number = 100, classProbs=TRUE, summaryFunction=twoClassSummary)

cvFit2 <- train(y ~ .,
                  data=HousingSubsidy2 %>% 
                    dplyr::select(-EnterProgram), 
                method="glm", family="binomial",
                metric="ROC", trControl = ctrl2)

cvFit2
```

```{r cv3}
ctrl3 <- trainControl(method = "cv", number = 100, classProbs=TRUE, summaryFunction=twoClassSummary)

cvFit3 <- train(y ~ .,
                  data=HousingSubsidy3 %>% 
                    dplyr::select(-EnterProgram), 
                method="glm", family="binomial",
                metric="ROC", trControl = ctrl3)

cvFit3
```

```{r goodness_metrics3, message = FALSE, warning = FALSE}
dplyr::select(cvFit3$resample, -Resample) %>%
  gather(metric, value) %>%
  left_join(gather(cvFit3$results[2:4], metric, mean)) %>%
  ggplot(aes(value)) + 
    geom_histogram(bins=35, fill = "#FF006A") +
    facet_wrap(~metric) +
    geom_vline(aes(xintercept = mean), colour = "#981FAC", linetype = 3, size = 1.5) +
    scale_x_continuous(limits = c(0, 1)) +
    labs(x="Goodness of Fit", y="Count", title="CV Goodness of Fit Metrics",
         subtitle = "Across-fold mean reprented as dotted lines")

```


### Develop a cost benefit analysis.

# Confusion Matrix

Each threshold (e.g. a probability above which a prediction is a "click" and below which it's a "no click") has it's own rate of error. These errors can be classified in four ways for a binary model.

A "confusion matrix" for the threshold of 50% shows us the rate at which we got True Positives (aka Sensitivity), False Positives, True Negatives (aka Specificity) and False Negatives for that threshold.

```{r thresholds}
testProbs3 <- 
  testProbs3 %>%
  mutate(predOutcome  = as.factor(ifelse(testProbs3$Probs > 0.5 , 1, 0)))
```

```{r confusion_matrix}
caret::confusionMatrix(testProbs3$predOutcome, testProbs3$Outcome, 
                       positive = "1")

```

# Cost-Benefit Calculation

### Create the ‘Cost/Benefit Table’

Write out the cost/benefit equation for each confusion metric.

We define negative value investments as the marketing and administrative costs associated with outreach to individuals. We define positive value investments as the awarding of a credit to an individual, minus the marketing and administrative cost associated with it. 
- True negative revenue “Predicted correctly homeowner would not take the credit, no marketing resources were allocated, and no credit was allocated.”: -\$0
- True positive revenue “Predicted correctly homeowner would enter credit program; allocated the marketing resources, and 25% ultimately achieved the credit”: -\$2,850 + \$5,000 = \$2,150 return for 25% of cases that take the credit. -\$2,850 for 75% of cases who did not take the credit.
- False negative revenue “We predicted that a homeowner would not take the credit but they did. These are likely homeowners who signed up for reasons unrelated to the marketing campaign”: \$0
- False positive revenue “Predicted incorrectly homeowner would take the credit; allocated marketing resources; no credit allocated.”: -\$2,850

```{r cost_benefit}
cost_benefit_table <-
   testProbs3 %>%
      count(predOutcome, Outcome) %>%
      summarize(True_Negative = sum(n[predOutcome==0 & Outcome==0]),
                True_Positive = sum(n[predOutcome==1 & Outcome==1]),
                False_Negative = sum(n[predOutcome==0 & Outcome==1]),
                False_Positive = sum(n[predOutcome==1 & Outcome==0])) %>%
       gather(Variable, Count) %>%
       mutate(Revenue =
               ifelse(Variable == "True_Negative", Count * 0,
               ifelse(Variable == "True_Positive", (Count * .25 * (-2850+5000)) + (Count * .75 * -2850),
               ifelse(Variable == "False_Negative", Count * 0,
               ifelse(Variable == "False_Positive", (Count * -2850), 0))))) %>%
    bind_cols(data.frame(Description = c(
              "We correctly predicted no enrollment",
              "We correctly predicted enrollment",
              "We predicted no enrollment and person enrolled",
              "We predicted enrollment and customer did not enroll")))

kable(cost_benefit_table,
       caption = "Cost/Benefit Table") %>% kable_styling()
```

### Plot the confusion metric outcomes for each Threshold.
```{r threshold}

whichThreshold <- 
  iterateThresholds(
     data=testProbs3, observedClass = Outcome, predictedProbs = Probs)

whichThreshold[1:5,]

whichThreshold <- 
  whichThreshold %>%
    dplyr::select(starts_with("Count"), Threshold) %>%
    gather(Variable, Count, -Threshold) %>%
    mutate(Revenue =
             case_when(Variable == "Count_TN"  ~ Count * 0,
                       Variable == "Count_TP"  ~ (Count * .25 * (-2850+5000)) + (Count * .75 * -2850) +
                                                 (-32 * (Count * .50)),
                       Variable == "Count_FN"  ~ Count * 0,
                       Variable == "Count_FP"  ~ (Count * -2850)))

whichThreshold %>%
  ggplot(.,aes(Threshold, Revenue, colour = Variable)) +
  geom_point() +
  scale_colour_manual(values = palette5[c(5, 1:3)]) +    
  labs(title = "Revenue by confusion matrix type and threshold",
       y = "Revenue") +
  plotTheme() +
  guides(colour=guide_legend(title = "Confusion Matrix")) 

```
### Create two small multiple plots that show Threshold as a function of Total_Revenue and Total_Count_of_Credits. Interpret this.

!!!!!These don't include false negatives FYI!!!!!

```{r threshold rev plot}

whichThreshold_revenue <- 
  whichThreshold %>% 
    mutate(Count = ifelse(Variable == "Count_TP", (Count * .25),0)) %>% 
    group_by(Threshold) %>% 
    summarize(Total_Revenue = sum(Revenue),
              Total_Count_of_Credits = sum(Count))

whichThreshold_revenue

fiftyThreshold = whichThreshold_revenue[50,]
optimalThreshold = whichThreshold_revenue[13,]
tableThreshold = rbind(fiftyThreshold, optimalThreshold)



rev_threshold <- whichThreshold_revenue %>%
  dplyr::select(Threshold, Total_Revenue) %>%
  gather(Variable, Value, -Threshold) %>%
  ggplot(aes(Threshold, Value, colour = Variable)) +
    geom_point() +
    geom_vline(xintercept = pull(arrange(optimalThreshold, -Total_Revenue)[1,1])) +
    scale_colour_manual(values = palette2) +
    plotTheme() +
    theme(legend.position = "none") +
    labs(title = "Total Investment Cost",
         subtitle = "Vertical line denotes chosen optimal threshold")

count_threshold <- whichThreshold_revenue %>%
  dplyr::select(Threshold, Total_Count_of_Credits) %>%
  gather(Variable, Value, -Threshold) %>%
  ggplot(aes(Threshold, Value, colour = Variable)) +
    geom_point() +
    geom_vline(xintercept = pull(arrange(optimalThreshold, -Total_Count_of_Credits)[1,1])) +
    scale_colour_manual(values = palette2) +
    plotTheme() +
    theme(legend.position = "none") +
    labs(title = "Total Count of Credits",
         subtitle = "Vertical line denotes chosen optimal credits")

plot_grid(rev_threshold, count_threshold, ncol = 1, align = "v")
```
### Create a table of the Total_Revenue and Total_Count_of_Credits allocated for 2 categories. 50%_Threshold and your Optimal_Threshold.

```{r table}

tableThreshold %>% kbl() %>% kable_minimal()

```


Conclude whether and why this model should or shouldn’t be put into production. What could make the model better? What would you do to ensure that the marketing materials 
resulted in a better response rate?
